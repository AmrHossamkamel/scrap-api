{"file_contents":{"README.md":{"content":"# üåê Advanced Web Scraping API\n\nA powerful, professional-grade web scraping platform built with FastAPI, featuring real-time streaming capabilities, intelligent content extraction, and comprehensive security measures.\n\n[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://python.org)\n[![FastAPI](https://img.shields.io/badge/FastAPI-0.68+-green.svg)](https://fastapi.tiangolo.com)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\n## ‚ú® Key Features\n\n### üöÄ Multiple Scraping Modes\n- **Single Page Extraction** - Precise content extraction from individual pages\n- **Limited Batch Scraping** - Controlled scraping with customizable page limits\n- **Real-time Streaming** - Live results with immediate page-by-page updates\n- **Unlimited Website Crawling** - Comprehensive site-wide content extraction\n\n### üõ°Ô∏è Enterprise-Grade Security\n- Advanced duplicate URL detection and normalization\n- Domain-restricted crawling for security compliance\n- Intelligent rate limiting and timeout controls\n- User-agent rotation to avoid bot detection\n- Request validation and sanitization\n\n### üì° Real-Time Capabilities\n- Server-Sent Events (SSE) streaming architecture\n- Live progress tracking and status updates\n- Non-blocking asynchronous processing\n- Interactive web interface with Arabic/RTL support\n\n### üéØ Advanced Content Processing\n- Enhanced HTML parsing with trafilatura integration\n- WordPress and modern CMS optimized extraction\n- Elementor and dynamic content support\n- Smart content filtering and cleanup\n\n## üèóÔ∏è Technical Architecture\n\n### Backend Stack\n- **Framework**: FastAPI with automatic OpenAPI documentation\n- **Language**: Python 3.11+ with modern async/await patterns\n- **HTTP Client**: Requests library with persistent session management\n- **Content Extraction**: BeautifulSoup4 + trafilatura for superior parsing\n- **Data Validation**: Pydantic models with automatic serialization\n- **Server**: Uvicorn ASGI with hot reload capabilities\n\n### Frontend Features\n- Responsive Arabic-RTL supporting interface\n- Real-time API health monitoring\n- Advanced form validation with live feedback\n- Results management with download/export functionality\n- 3D styling and modern animations\n\n### Core Design Principles\n- **Scalable Architecture**: Object-oriented design with separation of concerns\n- **Performance Optimized**: HTTP session pooling and connection reuse\n- **Security First**: Comprehensive input validation and domain restrictions\n- **User Experience**: Real-time feedback and intuitive interface design\n\n## üöÄ Quick Start\n\n### Prerequisites\n- Python 3.11 or higher\n- pip package manager\n\n### Installation & Setup\n\n1. **Clone the repository**\n```bash\ngit clone <repository-url>\ncd web-scraping-api\n```\n\n2. **Install dependencies**\n```bash\npip install -r requirements.txt\n```\n\n3. **Start the server**\n```bash\npython -m uvicorn main:app --host 0.0.0.0 --port 5000 --reload\n```\n\n4. **Access the application**\n- **Web Interface**: http://localhost:5000\n- **API Documentation**: http://localhost:5000/docs\n- **Alternative Docs**: http://localhost:5000/redoc\n\n## üìö API Endpoints\n\n### Core Scraping Endpoints\n\n#### `POST /scrape-pages`\nLimited scraping with page count control\n```json\n{\n    \"url\": \"https://example.com\",\n    \"max_pages\": 50,\n    \"timeout\": 10\n}\n```\n\n#### `POST /scrape-single`\nExtract content from a single page\n```json\n{\n    \"url\": \"https://example.com/article\",\n    \"timeout\": 10\n}\n```\n\n### Real-Time Streaming Endpoints\n\n#### `POST /scrape-stream`\nReal-time streaming with page limits\n```json\n{\n    \"url\": \"https://example.com\",\n    \"max_pages\": 100,\n    \"timeout\": 10\n}\n```\n\n#### `POST /scrape-stream-unlimited`\nUnlimited website crawling with live updates\n```json\n{\n    \"url\": \"https://example.com\",\n    \"timeout\": 10\n}\n```\n\n### Health & Monitoring\n\n#### `GET /health`\nAPI health check and status monitoring\n\n## üéÆ Web Interface Usage\n\n1. **Navigate to the web interface** at `http://localhost:5000`\n2. **Enter target URL** in the input field\n3. **Select scraping mode**:\n   - üéØ Single page only\n   - üìÑ Limited pages (specify count)\n   - üì° Real-time streaming\n   - üöÄ Unlimited website scraping\n4. **Configure settings** (max pages, timeout)\n5. **Start scraping** and monitor real-time progress\n6. **Download or export results** in JSON format\n\n## ‚öôÔ∏è Configuration Options\n\n### Scraping Parameters\n- **URL**: Target website URL (required)\n- **Max Pages**: Page limit for controlled scraping (1-500)\n- **Timeout**: Request timeout per page (5-60 seconds)\n\n### Security Controls\n- **Domain Restriction**: Automatic same-domain enforcement\n- **Rate Limiting**: Built-in request throttling\n- **Duplicate Prevention**: Advanced URL normalization\n- **Content Filtering**: Smart content extraction\n\n## üîß Advanced Features\n\n### URL Normalization & Deduplication\n- Removes tracking parameters (UTM, fbclid, etc.)\n- Handles URL fragments and anchors\n- Case-insensitive domain matching\n- Path normalization and canonicalization\n\n### Content Extraction Intelligence\n- WordPress-optimized parsing\n- Modern CMS compatibility\n- Dynamic content handling\n- Article structure detection\n\n### Performance Optimizations\n- HTTP connection pooling\n- Session management and reuse\n- Async/await processing\n- Memory-efficient streaming\n\n## üìä Response Format\n\n### Standard Response\n```json\n{\n    \"url\": \"https://example.com/page\",\n    \"title\": \"Page Title\",\n    \"content\": \"Extracted content...\",\n    \"links\": [\"https://example.com/link1\"],\n    \"timestamp\": \"2025-08-13T10:30:00Z\",\n    \"word_count\": 1250\n}\n```\n\n### Streaming Response Format\n```json\n{\n    \"type\": \"page\",\n    \"data\": {\n        \"url\": \"https://example.com/page\",\n        \"title\": \"Page Title\",\n        \"content\": \"Content...\",\n        \"links\": [\"...\"],\n        \"timestamp\": \"2025-08-13T10:30:00Z\"\n    },\n    \"progress\": {\n        \"current\": 15,\n        \"total\": 100,\n        \"percentage\": 15\n    }\n}\n```\n\n## üõ°Ô∏è Security & Compliance\n\n### Built-in Protections\n- ‚úÖ Domain validation and restriction\n- ‚úÖ Request rate limiting\n- ‚úÖ Input sanitization\n- ‚úÖ URL validation\n- ‚úÖ Timeout enforcement\n- ‚úÖ Memory usage controls\n\n### Best Practices\n- Always respect robots.txt\n- Implement appropriate delays between requests\n- Monitor server resources during large operations\n- Use reasonable page limits for batch operations\n\n## üö® Error Handling\n\nThe API provides comprehensive error responses:\n\n- **400 Bad Request**: Invalid URL or parameters\n- **404 Not Found**: Page or resource not accessible\n- **429 Too Many Requests**: Rate limit exceeded\n- **500 Internal Server Error**: Server processing error\n\n## üìà Performance Metrics\n\n### Typical Performance\n- **Single Page**: 1-3 seconds per page\n- **Batch Scraping**: 50-100 pages per minute\n- **Memory Usage**: ~50MB for standard operations\n- **Concurrent Requests**: Up to 10 simultaneous streams\n\n## ü§ù Contributing\n\nWe welcome contributions! Please follow these steps:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## üìÑ License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## üèÜ Credits\n\n**Developed by:** Eng. Amr Hossam  \n**Architecture:** FastAPI + Python 3.11+  \n**Frontend:** Modern JavaScript with Arabic RTL support  \n\n---\n\n## üîó Links\n\n- **Documentation**: [API Docs](http://localhost:5000/docs)\n- **Alternative Docs**: [ReDoc](http://localhost:5000/redoc)\n- **Health Check**: [Status](http://localhost:5000/health)\n\n---\n\n*Built with ‚ù§Ô∏è using FastAPI and modern web technologies*","size_bytes":7769},"main.py":{"content":"from fastapi import FastAPI, HTTPException, Query\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import FileResponse, StreamingResponse\nfrom typing import List, Dict, Any, Set, Generator, AsyncGenerator\nimport requests\nfrom bs4 import BeautifulSoup\nimport uuid\nfrom datetime import datetime, timezone\nfrom urllib.parse import urljoin, urlparse, urlunparse, parse_qs, urlencode\nimport logging\nfrom pydantic import BaseModel\nimport uvicorn\nimport os\nimport json\nimport asyncio\nimport trafilatura\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\napp = FastAPI(\n    title=\"Web Scraping API\",\n    description=\"A FastAPI-based web scraping API that crawls websites and extracts article content - Made by Eng: Amr Hossam\",\n    version=\"1.0.0\"\n)\n\n# Mount static files\nif os.path.exists(\"static\"):\n    app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\nclass ScrapedPage(BaseModel):\n    \"\"\"Model for scraped page data\"\"\"\n    data: Dict[str, Any]\n\nclass WebScraper:\n    def __init__(self, base_url: str, timeout: int = 10, max_pages: int = 100, callback=None):\n        self.base_url = base_url\n        self.timeout = timeout\n        self.max_pages = max_pages\n        self.visited_urls: Set[str] = set()\n        self.session = requests.Session()\n        self.callback = callback  # Callback function for streaming results\n        \n        # Set user agent to avoid blocking\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        })\n        \n        # Parse base URL to get domain\n        parsed_url = urlparse(base_url)\n        self.domain = parsed_url.netloc\n        self.scheme = parsed_url.scheme\n    \n    def is_same_domain(self, url: str) -> bool:\n        \"\"\"Check if URL belongs to the same domain\"\"\"\n        try:\n            parsed = urlparse(url)\n            return parsed.netloc == self.domain or parsed.netloc == f\"www.{self.domain}\" or parsed.netloc == self.domain.replace(\"www.\", \"\")\n        except Exception:\n            return False\n    \n    def normalize_url(self, url: str) -> str:\n        \"\"\"Normalize URL by removing fragments and sorting query parameters\"\"\"\n        try:\n            parsed = urlparse(url)\n            # Remove fragment and normalize\n            normalized = urlunparse((\n                parsed.scheme,\n                parsed.netloc,\n                parsed.path.rstrip('/') if parsed.path != '/' else parsed.path,\n                parsed.params,\n                parsed.query,\n                ''  # Remove fragment\n            ))\n            return normalized\n        except Exception:\n            return url\n    \n    def extract_links(self, html_content: str, base_url: str) -> Set[str]:\n        \"\"\"Extract all internal links from HTML content\"\"\"\n        links = set()\n        try:\n            soup = BeautifulSoup(html_content, 'html.parser')\n            \n            # Find all anchor tags with href attributes\n            for link in soup.find_all('a', href=True):\n                href_attr = link.get('href')\n                if not href_attr:\n                    continue\n                href = str(href_attr).strip()\n                if not href or href.startswith('#') or href.startswith('mailto:') or href.startswith('tel:'):\n                    continue\n                \n                # Convert relative URLs to absolute\n                absolute_url = urljoin(base_url, href)\n                \n                # Check if it's the same domain and not duplicate\n                if self.is_same_domain(absolute_url) and not self.is_duplicate_url(absolute_url):\n                    normalized_url = self.normalize_url(absolute_url)\n                    links.add(normalized_url)\n        \n        except Exception as e:\n            logger.error(f\"Error extracting links: {e}\")\n        \n        return links\n    \n    def normalize_url_for_deduplication(self, url: str) -> str:\n        \"\"\"Enhanced URL normalization for better duplicate detection\"\"\"\n        try:\n            parsed = urlparse(url.lower().strip())\n            \n            # Remove common tracking parameters\n            query_params = parse_qs(parsed.query)\n            filtered_params = {}\n            \n            # Keep only meaningful parameters, filter out tracking\n            tracking_params = {'utm_source', 'utm_medium', 'utm_campaign', 'utm_content', 'utm_term', \n                             'fbclid', 'gclid', 'ref', 'source', '_ga', '_gl', 'mc_cid', 'mc_eid',\n                             'campaign', 'medium', 'content', 'term', 'msclkid', 'wbraid', 'gbraid'}\n            \n            for key, value in query_params.items():\n                if key.lower() not in tracking_params:\n                    filtered_params[key] = value\n            \n            # Rebuild URL without tracking parameters\n            filtered_query = urlencode(filtered_params, doseq=True)\n            normalized_url = urlunparse((\n                parsed.scheme,\n                parsed.netloc,\n                parsed.path.rstrip('/'),  # Remove trailing slash\n                parsed.params,\n                filtered_query,\n                ''  # Remove fragment\n            ))\n            \n            return normalized_url\n            \n        except Exception as e:\n            logger.error(f\"Error normalizing URL for deduplication {url}: {e}\")\n            return url.lower().strip()\n    \n    def is_duplicate_url(self, url: str) -> bool:\n        \"\"\"Check if URL is duplicate using enhanced normalization\"\"\"\n        normalized = self.normalize_url_for_deduplication(url)\n        return normalized in self.visited_urls\n    \n    def extract_content(self, html_content: str, url: str) -> Dict[str, Any]:\n        \"\"\"Extract title and content from HTML using comprehensive BeautifulSoup method\"\"\"\n        try:\n            # Use BeautifulSoup for complete content extraction\n            soup = BeautifulSoup(html_content, 'html.parser')\n            \n            # Extract title\n            title = \"\"\n            title_tag = soup.find('title')\n            if title_tag:\n                title = title_tag.get_text().strip()\n            else:\n                h1_tag = soup.find('h1')\n                if h1_tag:\n                    title = h1_tag.get_text().strip()\n            \n            # Remove only script and style elements, keep everything else for complete content\n            for script in soup([\"script\", \"style\"]):\n                script.decompose()\n            \n            # Extract complete page content using comprehensive method\n            # Use complete body text to ensure no content is missed\n            body = soup.find('body')\n            if body:\n                content = body.get_text(separator=' ', strip=True)\n            else:\n                content = soup.get_text(separator=' ', strip=True)\n            \n            # Clean up content - remove extra whitespace\n            content = ' '.join(content.split())\n            \n            # Generate unique ID and timestamp\n            page_id = str(uuid.uuid4())\n            created_at = datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')\n            \n            return {\n                \"created_at\": created_at,\n                \"id\": page_id,\n                \"source_url\": url,\n                \"title\": title,\n                \"content\": content\n            }\n        \n        except Exception as e:\n            logger.error(f\"Error extracting content from {url}: {e}\")\n            return {\n                \"created_at\": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),\n                \"id\": str(uuid.uuid4()),\n                \"source_url\": url,\n                \"title\": \"\",\n                \"content\": f\"Error extracting content: {str(e)}\"\n            }\n    \n    def scrape_page(self, url: str) -> Dict[str, Any]:\n        \"\"\"Scrape a single page and return extracted data\"\"\"\n        try:\n            logger.info(f\"Scraping: {url}\")\n            response = self.session.get(url, timeout=self.timeout)\n            response.raise_for_status()\n            \n            # Check if content type is HTML\n            content_type = response.headers.get('content-type', '').lower()\n            if 'text/html' not in content_type:\n                logger.warning(f\"Skipping non-HTML content: {url}\")\n                return {\n                    \"created_at\": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),\n                    \"id\": str(uuid.uuid4()),\n                    \"source_url\": url,\n                    \"title\": \"Non-HTML Content\",\n                    \"content\": \"Skipped non-HTML content\"\n                }\n            \n            return self.extract_content(response.text, url)\n        \n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Request error for {url}: {e}\")\n            return {\n                \"created_at\": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),\n                \"id\": str(uuid.uuid4()),\n                \"source_url\": url,\n                \"title\": \"\",\n                \"content\": f\"Request error: {str(e)}\"\n            }\n        except Exception as e:\n            logger.error(f\"Unexpected error for {url}: {e}\")\n            return {\n                \"created_at\": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),\n                \"id\": str(uuid.uuid4()),\n                \"source_url\": url,\n                \"title\": \"\",\n                \"content\": f\"Unexpected error: {str(e)}\"\n            }\n    \n    def crawl_website(self) -> List[Dict[str, Any]]:\n        \"\"\"Crawl the entire website and return scraped data\"\"\"\n        scraped_data = []\n        urls_to_visit = [self.normalize_url(self.base_url)]\n        \n        while urls_to_visit and len(scraped_data) < self.max_pages:\n            current_url = urls_to_visit.pop(0)\n            \n            # Skip if already visited\n            if current_url in self.visited_urls:\n                continue\n            \n            # Mark as visited\n            self.visited_urls.add(current_url)\n            \n            # Scrape the page\n            page_data = self.scrape_page(current_url)\n            if page_data:\n                scraped_data.append(page_data)\n                \n                # Call callback if streaming is enabled\n                if self.callback:\n                    self.callback(page_data)\n                \n                # Extract links only if we successfully scraped the page\n                try:\n                    response = self.session.get(current_url, timeout=self.timeout)\n                    if response.status_code == 200 and 'text/html' in response.headers.get('content-type', '').lower():\n                        new_links = self.extract_links(response.text, current_url)\n                        \n                        # Add new links to visit queue\n                        for link in new_links:\n                            if link not in self.visited_urls and link not in urls_to_visit:\n                                urls_to_visit.append(link)\n                \n                except Exception as e:\n                    logger.error(f\"Error extracting links from {current_url}: {e}\")\n        \n        logger.info(f\"Crawling completed. Scraped {len(scraped_data)} pages.\")\n        return scraped_data\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Serve the main UI\"\"\"\n    if os.path.exists(\"static/index.html\"):\n        return FileResponse(\"static/index.html\")\n    else:\n        # Fallback API information if static files don't exist\n        return {\n            \"message\": \"Web Scraping API\",\n            \"description\": \"A FastAPI-based web scraping API that crawls websites and extracts article content - Made by Eng: Amr Hossam\",\n            \"endpoints\": {\n                \"/scrape\": \"POST - Scrape a website for article content\",\n                \"/docs\": \"GET - API documentation\"\n            }\n        }\n\n@app.get(\"/api\", tags=[\"System Info\"])\nasync def api_info():\n    \"\"\"API endpoint information and available routes\"\"\"\n    return {\n        \"message\": \"üï∑Ô∏è Web Scraping API\",\n        \"description\": \"Advanced web scraping API that crawls websites and extracts article content - Made by Eng: Amr Hossam\",\n        \"endpoints\": {\n            \"/scrape\": \"üîç POST - Scrape limited number of pages\",\n            \"/scrape-single\": \"üéØ POST - Scrape single page only\",\n            \"/scrape-all\": \"üöÄ POST - Scrape ALL pages (unlimited)\",\n            \"/docs\": \"üìö GET - Interactive API documentation\",\n            \"/health\": \"üíö GET - System health check\",\n            \"/api\": \"üìä GET - API information\"\n        },\n        \"features\": [\n            \"üï∏Ô∏è Automatic link discovery\",\n            \"üõ°Ô∏è Safe with duplicate prevention\", \n            \"üìä Smart content extraction\",\n            \"‚ö° High performance with HTTP sessions\",\n            \"üéØ Support for limited and unlimited scraping\"\n        ],\n        \"developer\": \"üë®‚Äçüíª Eng: Amr Hossam\",\n        \"version\": \"1.0.0\"\n    }\n\n@app.post(\"/scrape-single\", response_model=ScrapedPage, tags=[\"Web Scraping\"])\nasync def scrape_single_page(\n    url: str = Query(\n        ..., \n        description=\"The URL of the specific page to scrape\",\n        example=\"https://example.com/article\"\n    ),\n    timeout: int = Query(\n        10, \n        description=\"Request timeout in seconds for the page\", \n        ge=1, \n        le=60,\n        example=10\n    )\n):\n    \"\"\"\n    **Extract content from a single page only**\n    \n    This endpoint scrapes only the specified page without following any links.\n    Perfect for extracting content from a specific article or page.\n    \n    ## Features:\n    - üéØ **Single Page**: Only scrapes the exact URL provided\n    - ‚ö° **Fast**: No link discovery, direct page scraping\n    - üìä **Smart Extraction**: Same content extraction as full scraping\n    - üõ°Ô∏è **Safe**: No risk of infinite crawling\n    \n    ## Parameters:\n    - **url**: Exact URL of the page to scrape\n    - **timeout**: Timeout in seconds for the request\n    \n    ## Returns:\n    A single scraped page containing:\n    - Title and content\n    - Source URL\n    - Extraction timestamp\n    - Unique ID\n    \n    ## Usage:\n    Perfect for scraping specific articles, blog posts, or individual pages\n    when you don't need the entire website.\n    \"\"\"\n    \n    # Validate URL\n    try:\n        parsed_url = urlparse(url)\n        if not parsed_url.scheme or not parsed_url.netloc:\n            raise HTTPException(status_code=400, detail=\"Invalid URL format\")\n        \n        if parsed_url.scheme not in ['http', 'https']:\n            raise HTTPException(status_code=400, detail=\"URL must use HTTP or HTTPS protocol\")\n    \n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid URL: {str(e)}\")\n    \n    try:\n        # Initialize scraper just to use its scrape_page method\n        scraper = WebScraper(url, timeout=timeout, max_pages=1)\n        \n        # Scrape only the single page\n        page_data = scraper.scrape_page(url)\n        \n        if not page_data or not page_data.get('content'):\n            raise HTTPException(status_code=404, detail=\"No content could be extracted from the provided URL\")\n        \n        # Format response according to specified structure\n        return {\n            \"data\": page_data\n        }\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error during single page scraping: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Internal server error: {str(e)}\")\n\n@app.post(\"/scrape-all\", response_model=List[ScrapedPage], tags=[\"Web Scraping\"])\nasync def scrape_all_pages(\n    url: str = Query(\n        ..., \n        description=\"The base URL of the website to scrape completely\",\n        example=\"https://example.com\"\n    ),\n    timeout: int = Query(\n        10, \n        description=\"Request timeout in seconds for each page\", \n        ge=1, \n        le=60,\n        example=10\n    )\n):\n    \"\"\"\n    **Extract content from ALL pages on a website with no limit**\n    \n    This endpoint scrapes the entire website by following all internal links\n    without any page limit. Perfect for complete website content extraction.\n    \n    ## Features:\n    - üï∏Ô∏è **Complete Discovery**: Finds and scrapes every internal page\n    - ‚ôæÔ∏è **No Limits**: Continues until all pages are discovered\n    - üõ°Ô∏è **Domain Safe**: Only follows links within the same domain\n    - üìä **Smart Extraction**: Same content extraction as other endpoints\n    - üîÑ **Duplicate Prevention**: Avoids visiting the same page twice\n    \n    ## Parameters:\n    - **url**: Base URL of the website to scrape completely\n    - **timeout**: Timeout in seconds for each page request\n    \n    ## Returns:\n    A list of ALL scraped pages from the website, each containing:\n    - Title and content\n    - Source URL\n    - Extraction timestamp\n    - Unique ID\n    \n    ## Warning:\n    This may take a very long time for large websites and will scrape\n    every discoverable page. Use with caution on very large sites.\n    \"\"\"\n    \n    # Validate URL\n    try:\n        parsed_url = urlparse(url)\n        if not parsed_url.scheme or not parsed_url.netloc:\n            raise HTTPException(status_code=400, detail=\"Invalid URL format\")\n        \n        if parsed_url.scheme not in ['http', 'https']:\n            raise HTTPException(status_code=400, detail=\"URL must use HTTP or HTTPS protocol\")\n    \n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid URL: {str(e)}\")\n    \n    try:\n        # Initialize scraper with very high max_pages for unlimited scraping\n        scraper = WebScraper(url, timeout=timeout, max_pages=999999)\n        \n        # Crawl entire website\n        scraped_data = scraper.crawl_website()\n        \n        if not scraped_data:\n            raise HTTPException(status_code=404, detail=\"No content could be scraped from the provided URL\")\n        \n        # Format response according to specified structure\n        formatted_response = []\n        for page_data in scraped_data:\n            formatted_response.append({\n                \"data\": page_data\n            })\n        \n        return formatted_response\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error during unlimited scraping: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Internal server error: {str(e)}\")\n\n@app.post(\"/scrape-pages\", response_model=List[ScrapedPage], tags=[\"Web Scraping\"])\nasync def scrape_website(\n    url: str = Query(\n        ..., \n        description=\"The base URL of the website to scrape\",\n        example=\"https://example.com\"\n    ),\n    max_pages: int = Query(\n        100, \n        description=\"Maximum number of pages to scrape. Use 999999 or higher for unlimited scraping\", \n        ge=1, \n        le=999999,\n        example=50\n    ),\n    timeout: int = Query(\n        10, \n        description=\"Request timeout in seconds for each page\", \n        ge=1, \n        le=60,\n        example=10\n    )\n):\n    \"\"\"\n    **Extract content from websites**\n    \n    This API visits the specified website and extracts content from all internal pages.\n    \n    ## Features:\n    - üï∏Ô∏è **Auto Discovery**: Follows all internal links automatically\n    - üõ°Ô∏è **Safe**: Stays within the specified domain only\n    - üìä **Smart Extraction**: Cleans content and extracts titles and text\n    - ‚ö° **Fast**: Uses persistent HTTP sessions for performance\n    - üîÑ **Duplicate Prevention**: Avoids visiting the same page twice\n    \n    ## Parameters:\n    - **url**: Base URL of the website (must start with http or https)\n    - **max_pages**: Maximum number of pages to scrape (use 999999+ for unlimited)\n    - **timeout**: Timeout in seconds for each page request\n    \n    ## Returns:\n    A list of scraped pages, each containing:\n    - Title and content\n    - Source URL\n    - Extraction timestamp\n    - Unique ID\n    \n    ## Usage Examples:\n    - **Limited scraping**: `max_pages=50` to scrape only 50 pages\n    - **Unlimited scraping**: `max_pages=999999` to scrape all pages on the website\n    \"\"\"\n    \n    # Validate URL\n    try:\n        parsed_url = urlparse(url)\n        if not parsed_url.scheme or not parsed_url.netloc:\n            raise HTTPException(status_code=400, detail=\"Invalid URL format\")\n        \n        if parsed_url.scheme not in ['http', 'https']:\n            raise HTTPException(status_code=400, detail=\"URL must use HTTP or HTTPS protocol\")\n    \n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid URL: {str(e)}\")\n    \n    try:\n        # Initialize scraper\n        scraper = WebScraper(url, timeout=timeout, max_pages=max_pages)\n        \n        # Crawl website\n        scraped_data = scraper.crawl_website()\n        \n        if not scraped_data:\n            raise HTTPException(status_code=404, detail=\"No content could be scraped from the provided URL\")\n        \n        # Format response according to specified structure\n        formatted_response = []\n        for page_data in scraped_data:\n            formatted_response.append({\n                \"data\": page_data\n            })\n        \n        return formatted_response\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error during scraping: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Internal server error: {str(e)}\")\n\n@app.post(\"/scrape-stream\", \n        tags=[\"üì° Real-time Streaming\"], \n        summary=\"Stream scraped pages in real-time\",\n        description=\"\"\"\n## Real-time Streaming Web Scraper\n\nStream scraped pages in real-time with immediate results.\nEach page is sent as soon as it's processed, providing live updates.\n\n### Features:\n- **üì° Real-time Streaming**: Results appear as soon as each page is scraped\n- **üöÄ No Waiting**: See pages immediately instead of waiting for completion  \n- **üìä Live Progress**: Track scraping progress in real-time\n- **üõ°Ô∏è Same Security**: All security features of regular scraping\n- **üéØ Limited Pages**: Control the number of pages to extract\n\n### Usage:\nPerfect for interactive applications where users want to see results immediately.\nReturns streaming JSON responses with real-time updates.\n\n### Request Body:\n```json\n{\n    \"url\": \"https://example.com\",\n    \"max_pages\": 50,\n    \"timeout\": 10\n}\n```\n        \"\"\")\nasync def scrape_website_stream(request: dict):\n    \n    # Extract parameters from request\n    url = request.get('url')\n    max_pages = request.get('max_pages', 50)\n    timeout = request.get('timeout', 10)\n    \n    # Validate parameters\n    if not url:\n        raise HTTPException(status_code=400, detail=\"URL is required\")\n    \n    # Validate URL\n    try:\n        parsed_url = urlparse(url)\n        if not parsed_url.scheme or not parsed_url.netloc:\n            raise HTTPException(status_code=400, detail=\"Invalid URL format\")\n        \n        if parsed_url.scheme not in ['http', 'https']:\n            raise HTTPException(status_code=400, detail=\"URL must use HTTP or HTTPS protocol\")\n    \n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid URL: {str(e)}\")\n    \n    async def generate_stream() -> Generator[str, None, None]:\n        \"\"\"Generate SSE stream for real-time scraping results\"\"\"\n        scraped_count = 0\n        \n        def stream_callback(page_data):\n            nonlocal scraped_count\n            scraped_count += 1\n            \n            # Format the page data for streaming\n            event_data = {\n                \"type\": \"page\",\n                \"data\": page_data,\n                \"progress\": {\n                    \"current\": scraped_count,\n                    \"total\": max_pages,\n                    \"percentage\": min(100, (scraped_count / max_pages) * 100)\n                }\n            }\n            \n            return f\"data: {json.dumps(event_data, ensure_ascii=False)}\\n\\n\"\n        \n        try:\n            # Send start event\n            start_event = {\n                \"type\": \"start\",\n                \"message\": \"ÿ®ÿØÿ° ÿπŸÖŸÑŸäÿ© ÿßŸÑÿ≥ŸÉÿ±ÿßÿ®ŸÜÿ¨...\",\n                \"timestamp\": datetime.now(timezone.utc).isoformat()\n            }\n            yield f\"data: {json.dumps(start_event, ensure_ascii=False)}\\n\\n\"\n            \n            # Initialize scraper with callback for streaming\n            scraper = WebScraper(url, timeout=timeout, max_pages=max_pages)\n            \n            # Store streamed results\n            streamed_results = []\n            \n            def capture_result(page_data):\n                streamed_results.append(page_data)\n                # Generate stream event\n                return stream_callback(page_data)\n            \n            scraper.callback = capture_result\n            \n            # Start crawling and stream results\n            urls_to_visit = [scraper.normalize_url(scraper.base_url)]\n            \n            while urls_to_visit and len(streamed_results) < max_pages:\n                current_url = urls_to_visit.pop(0)\n                \n                # Skip if already visited\n                if current_url in scraper.visited_urls:\n                    continue\n                \n                # Mark as visited\n                scraper.visited_urls.add(current_url)\n                \n                # Scrape the page\n                page_data = scraper.scrape_page(current_url)\n                if page_data:\n                    streamed_results.append(page_data)\n                    \n                    # Stream the result immediately\n                    event_data = {\n                        \"type\": \"page\",\n                        \"data\": page_data,\n                        \"progress\": {\n                            \"current\": len(streamed_results),\n                            \"total\": max_pages,\n                            \"percentage\": min(100, (len(streamed_results) / max_pages) * 100)\n                        }\n                    }\n                    yield f\"data: {json.dumps(event_data, ensure_ascii=False)}\\n\\n\"\n                    \n                    # Extract links for next pages\n                    try:\n                        response = scraper.session.get(current_url, timeout=timeout)\n                        if response.status_code == 200 and 'text/html' in response.headers.get('content-type', '').lower():\n                            new_links = scraper.extract_links(response.text, current_url)\n                            \n                            # Add new links to visit queue\n                            for link in new_links:\n                                if link not in scraper.visited_urls and link not in urls_to_visit:\n                                    urls_to_visit.append(link)\n                    \n                    except Exception as e:\n                        logger.error(f\"Error extracting links from {current_url}: {e}\")\n                \n                # Small delay to prevent overwhelming the client and reduce duplicate processing\n                await asyncio.sleep(0.2)\n            \n            # Send completion event\n            complete_event = {\n                \"type\": \"complete\",\n                \"message\": f\"ÿ™ŸÖ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°! ÿ™ŸÖ ÿ≥ŸÉÿ±ÿßÿ®ŸÜÿ¨ {len(streamed_results)} ÿµŸÅÿ≠ÿ©\",\n                \"total_pages\": len(streamed_results),\n                \"timestamp\": datetime.now(timezone.utc).isoformat()\n            }\n            yield f\"data: {json.dumps(complete_event, ensure_ascii=False)}\\n\\n\"\n        \n        except Exception as e:\n            logger.error(f\"Error during streaming scrape: {e}\")\n            error_event = {\n                \"type\": \"error\",\n                \"message\": f\"ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£: {str(e)}\",\n                \"timestamp\": datetime.now(timezone.utc).isoformat()\n            }\n            yield f\"data: {json.dumps(error_event, ensure_ascii=False)}\\n\\n\"\n    \n    return StreamingResponse(\n        generate_stream(),\n        media_type=\"text/plain; charset=utf-8\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"Content-Type\": \"text/event-stream\",\n            \"Access-Control-Allow-Origin\": \"*\",\n            \"Access-Control-Allow-Methods\": \"GET, POST\",\n            \"Access-Control-Allow-Headers\": \"Content-Type\"\n        }\n    )\n\n@app.post(\"/scrape-stream-unlimited\", \n        tags=[\"üöÄ Unlimited Streaming\"], \n        summary=\"Stream unlimited website scraping\",\n        description=\"\"\"\n## Unlimited Website Scraper with Real-time Streaming\n\nThis endpoint scrapes entire websites without page limits. \nResults are streamed in real-time with immediate updates.\n\n### Features:\n- **üåê Complete Coverage**: Continues until all discoverable pages are scraped\n- **üì° Real-time Stream**: Results appear as soon as each page is processed  \n- **üõ°Ô∏è Advanced Protection**: Smart duplicate prevention and infinite loop protection\n- **‚ö° High Performance**: Uses HTTP sessions and network optimizations\n- **üîç Smart Discovery**: Automatically follows internal links\n\n### Warning:\n‚ö†Ô∏è **Caution**: May take a very long time for large websites and consume significant resources\n\n### Usage:\nPerfect for websites where you need comprehensive content extraction.\nReturns streaming JSON responses with real-time updates.\n\n### Request Body:\n```json\n{\n    \"url\": \"https://example.com\",\n    \"timeout\": 10\n}\n```\n        \"\"\")\nasync def scrape_stream_unlimited(request: dict):\n    \n    # Extract parameters from request\n    url = request.get('url')\n    timeout = request.get('timeout', 10)\n    \n    # Validate parameters\n    if not url:\n        raise HTTPException(status_code=400, detail=\"URL is required\")\n    \n    # Validate URL\n    try:\n        parsed_url = urlparse(url)\n        if not parsed_url.scheme or not parsed_url.netloc:\n            raise HTTPException(status_code=400, detail=\"Invalid URL format\")\n        \n        if parsed_url.scheme not in ['http', 'https']:\n            raise HTTPException(status_code=400, detail=\"URL must use HTTP or HTTPS protocol\")\n    \n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid URL: {str(e)}\")\n    \n    async def generate_unlimited_stream() -> Generator[str, None, None]:\n        \"\"\"Generate SSE stream for unlimited real-time scraping results\"\"\"\n        scraped_count = 0\n        \n        try:\n            # Send start event\n            start_event = {\n                \"type\": \"start\",\n                \"message\": \"ÿ®ÿØÿ° ÿπŸÖŸÑŸäÿ© ÿßŸÑÿ≥ŸÉÿ±ÿßÿ®ŸÜÿ¨ ÿßŸÑÿ¥ÿßŸÖŸÑ ŸÑŸÑŸÖŸàŸÇÿπ...\",\n                \"timestamp\": datetime.now(timezone.utc).isoformat()\n            }\n            yield f\"data: {json.dumps(start_event, ensure_ascii=False)}\\n\\n\"\n            \n            # Initialize scraper without page limit\n            scraper = WebScraper(url, timeout=timeout, max_pages=999999)\n            \n            # Store streamed results\n            streamed_results = []\n            \n            # Start crawling and stream results\n            urls_to_visit = [scraper.normalize_url(scraper.base_url)]\n            \n            while urls_to_visit:\n                current_url = urls_to_visit.pop(0)\n                \n                # Skip if already visited (with enhanced duplicate detection)\n                normalized_for_check = scraper.normalize_url_for_deduplication(current_url)\n                if normalized_for_check in scraper.visited_urls:\n                    continue\n                \n                # Mark as visited (using enhanced normalization)\n                scraper.visited_urls.add(normalized_for_check)\n                \n                # Scrape the page\n                page_data = scraper.scrape_page(current_url)\n                if page_data:\n                    scraped_count += 1\n                    streamed_results.append(page_data)\n                    \n                    # Stream the result immediately\n                    event_data = {\n                        \"type\": \"page\",\n                        \"data\": page_data,\n                        \"progress\": {\n                            \"current\": scraped_count,\n                            \"total\": \"ÿ∫Ÿäÿ± ŸÖÿ≠ÿØŸàÿØ\",\n                            \"percentage\": None,\n                            \"queue_size\": len(urls_to_visit)\n                        }\n                    }\n                    yield f\"data: {json.dumps(event_data, ensure_ascii=False)}\\n\\n\"\n                    \n                    # Extract links for next pages\n                    try:\n                        response = scraper.session.get(current_url, timeout=timeout)\n                        if response.status_code == 200 and 'text/html' in response.headers.get('content-type', '').lower():\n                            new_links = scraper.extract_links(response.text, current_url)\n                            \n                            # Add new links to visit queue\n                            for link in new_links:\n                                if link not in scraper.visited_urls and link not in urls_to_visit:\n                                    urls_to_visit.append(link)\n                    \n                    except Exception as e:\n                        logger.error(f\"Error extracting links from {current_url}: {e}\")\n                \n                # Small delay to prevent overwhelming the client\n                await asyncio.sleep(0.2)\n                \n                # Enhanced URL validation to prevent duplicates and improve efficiency\n                if scraped_count > 0 and scraped_count % 1000 == 0:\n                    progress_event = {\n                        \"type\": \"progress\",\n                        \"message\": f\"ÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ {scraped_count} ÿµŸÅÿ≠ÿ©... ÿßÿ≥ÿ™ŸÖÿ±ÿßÿ± ÿßŸÑÿπŸÖŸÑ\",\n                        \"current\": scraped_count,\n                        \"timestamp\": datetime.now(timezone.utc).isoformat()\n                    }\n                    yield f\"data: {json.dumps(progress_event, ensure_ascii=False)}\\n\\n\"\n            \n            # Send completion event\n            complete_event = {\n                \"type\": \"complete\",\n                \"message\": f\"ÿ™ŸÖ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ° ŸÖŸÜ ÿßŸÑÿ≥ŸÉÿ±ÿßÿ®ŸÜÿ¨ ÿßŸÑÿ¥ÿßŸÖŸÑ! ÿ™ŸÖ ÿ≥ŸÉÿ±ÿßÿ®ŸÜÿ¨ {scraped_count} ÿµŸÅÿ≠ÿ©\",\n                \"total_pages\": scraped_count,\n                \"timestamp\": datetime.now(timezone.utc).isoformat()\n            }\n            yield f\"data: {json.dumps(complete_event, ensure_ascii=False)}\\n\\n\"\n        \n        except Exception as e:\n            logger.error(f\"Error during unlimited streaming scrape: {e}\")\n            error_event = {\n                \"type\": \"error\",\n                \"message\": f\"ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£: {str(e)}\",\n                \"timestamp\": datetime.now(timezone.utc).isoformat()\n            }\n            yield f\"data: {json.dumps(error_event, ensure_ascii=False)}\\n\\n\"\n    \n    return StreamingResponse(\n        generate_unlimited_stream(),\n        media_type=\"text/plain; charset=utf-8\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"Content-Type\": \"text/event-stream\",\n            \"Access-Control-Allow-Origin\": \"*\",\n            \"Access-Control-Allow-Methods\": \"GET, POST\",\n            \"Access-Control-Allow-Headers\": \"Content-Type\"\n        }\n    )\n\n@app.get(\"/database\")\nasync def database_page():\n    \"\"\"Serve the database management UI\"\"\"\n    if os.path.exists(\"static/database.html\"):\n        return FileResponse(\"static/database.html\")\n    else:\n        raise HTTPException(status_code=404, detail=\"Database management page not found\")\n\n@app.get(\"/health\", tags=[\"System Info\"])\nasync def health_check():\n    \"\"\"üíö System health check and API readiness verification\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"message\": \"‚úÖ System is running normally\",\n        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n        \"uptime\": \"Available 24/7\",\n        \"version\": \"1.0.0\"\n    }\n\nif __name__ == \"__main__\":\n    # Run the application\n    uvicorn.run(\n        \"main:app\",\n        host=\"0.0.0.0\",\n        port=5000,\n        reload=True,\n        log_level=\"info\"\n    )\n","size_bytes":36219},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"beautifulsoup4>=4.13.4\",\n    \"fastapi>=0.116.1\",\n    \"pydantic>=2.11.7\",\n    \"requests>=2.32.4\",\n    \"trafilatura>=2.0.0\",\n    \"uvicorn>=0.35.0\",\n]\n","size_bytes":295},"replit.md":{"content":"# Web Scraping API\n\n## Overview\n\nThis project is a FastAPI-based web scraping service that crawls websites and extracts article content. The application provides REST API endpoints for initiating web scraping operations on specified domains with built-in safety controls and rate limiting. It features a modern, Arabic-supporting web interface for managing scraping operations and includes comprehensive security measures to prevent abuse.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## Recent Changes\n\n### August 13, 2025\n- Successfully completed migration from Replit Agent to Replit environment\n- Fixed uvicorn dependency and import issues for seamless operation\n- Enhanced Swagger documentation for all endpoints with professional English descriptions\n- Renamed main scrape endpoint from `/scrape` to `/scrape-pages` for better clarity\n- Converted streaming endpoints from GET to POST method per user request\n- Updated JavaScript to use fetch with streaming readers instead of EventSource for POST compatibility\n- Improved streaming endpoint documentation with clear request body examples\n- Fixed JavaScript duplicate handling for unlimited streaming mode\n- All streaming endpoints now use POST method with JSON request bodies\n- All security features and duplicate detection systems working properly\n\n### August 11, 2025\n- Successfully migrated project from Replit Agent to Replit environment\n- Fixed uvicorn dependency issue and FastAPI server configuration\n- Added real-time streaming functionality using Server-Sent Events (SSE)\n- Implemented /scrape-stream endpoint for live result updates\n- Enhanced frontend with streaming support and real-time progress display\n- Fixed EventSource method compatibility (GET instead of POST)\n- User requested live streaming of scraping results instead of waiting for completion\n- User feedback: Keep same deduplication system but show pages as they complete\n- Maintained existing duplicate prevention while adding real-time display\n- Enhanced content extraction using trafilatura library for better WordPress and modern website support\n- Improved extraction for sites with dynamic content loading and Elementor-based layouts\n- Added unlimited streaming endpoint (/scrape-stream-unlimited) for comprehensive website crawling\n- User requested: Create unlimited streaming endpoint that continues until all website pages are scraped\n- Removed safety limit per user request, added enhanced URL deduplication instead\n- Enhanced URL normalization to filter tracking parameters and prevent duplicate scraping\n- User feedback: Remove limits for unlimited streaming, focus on better duplicate detection\n\n## System Architecture\n\n### Backend Architecture\n- **Framework**: FastAPI for REST API development with automatic OpenAPI documentation\n- **Language**: Python 3.x with type hints and modern async/await patterns\n- **HTTP Client**: Requests library with session management for connection pooling and persistent connections\n- **HTML Parsing**: BeautifulSoup4 for robust content extraction and DOM manipulation\n- **Data Models**: Pydantic models for request/response validation, serialization, and automatic schema generation\n\n### Core Design Patterns\n- **Object-Oriented Design**: WebScraper class encapsulates scraping logic, state management, and configuration\n- **Session Management**: Persistent HTTP sessions for improved performance and connection reuse across requests\n- **Rate Limiting**: Built-in timeout controls and maximum page limits to prevent resource abuse and infinite crawling\n- **Domain Validation**: URL parsing and domain matching to ensure scraping operations stay within specified boundaries\n- **State Tracking**: Visited URL tracking with sets to prevent duplicate requests and infinite loops\n\n### Security & Safety Features\n- **User Agent Spoofing**: Mimics legitimate browser headers to avoid bot detection and access restrictions\n- **Domain Restriction**: Enforces same-domain crawling to prevent unauthorized cross-domain scraping\n- **Request Limits**: Configurable maximum page limits and timeout controls to prevent server overload\n- **URL Deduplication**: Comprehensive visited URL tracking to prevent infinite loops and duplicate processing\n- **Input Validation**: Pydantic model validation for all API inputs and outputs\n\n### Frontend Architecture\n- **Modern UI**: Responsive Arabic-RTL supporting interface with 3D styling and animations\n- **Real-time Status**: Live API health monitoring and operation status updates\n- **Form Validation**: Client-side input validation with real-time feedback\n- **Results Management**: Download functionality and result visualization with clear/export options\n- **Progressive Enhancement**: Works without JavaScript but enhanced experience with it enabled\n\n### Data Processing\n- **Content Extraction**: Advanced HTML parsing with BeautifulSoup for clean content extraction\n- **URL Normalization**: Proper URL joining, parsing, and canonicalization for link following\n- **Structured Output**: Consistent JSON responses with Pydantic model validation\n- **Error Handling**: Comprehensive exception handling with meaningful error messages\n\n## External Dependencies\n\n### Core Backend Libraries\n- **FastAPI**: Modern web framework for building REST APIs with automatic documentation\n- **Requests**: HTTP library for making web requests with session management\n- **BeautifulSoup4**: HTML/XML parsing library for content extraction\n- **Pydantic**: Data validation and serialization using Python type annotations\n- **Uvicorn**: Lightning-fast ASGI server for running FastAPI applications\n\n### Frontend Dependencies\n- **Google Fonts**: Cairo font family for Arabic text support\n- **Modern CSS**: CSS Grid, Flexbox, and CSS custom properties for responsive design\n- **Vanilla JavaScript**: No framework dependencies, pure JavaScript for API interaction\n\n### Python Standard Library\n- **urllib.parse**: URL parsing, joining, and manipulation utilities\n- **datetime**: Timestamp generation and timezone handling for request tracking\n- **logging**: Structured logging for debugging and monitoring\n- **uuid**: Unique identifier generation for request tracking\n- **os**: File system operations for static file serving","size_bytes":6240},"static/database.js":{"content":"// Database Management UI - JavaScript Controller\nclass DatabaseManager {\n    constructor() {\n        this.endpoint = '';\n        this.secretKey = '';\n        this.isConnected = false;\n        \n        this.init();\n    }\n    \n    init() {\n        this.bindEvents();\n        this.loadSavedConfig();\n        this.checkForScrapingResults();\n        this.loadSavedQueries();\n    }\n    \n    bindEvents() {\n        // Database configuration form\n        const dbConfigForm = document.getElementById('dbConfigForm');\n        dbConfigForm.addEventListener('submit', (e) => this.handleConfigSubmit(e));\n        \n        // Query execution form\n        const queryForm = document.getElementById('queryForm');\n        queryForm.addEventListener('submit', (e) => this.handleQuerySubmit(e));\n        \n        // Action buttons\n        const downloadResultsBtn = document.getElementById('downloadResultsBtn');\n        const clearResultsBtn = document.getElementById('clearResultsBtn');\n        \n        downloadResultsBtn.addEventListener('click', () => this.downloadResults());\n        clearResultsBtn.addEventListener('click', () => this.clearResults());\n        \n        // Save/Load query buttons\n        const saveQueryBtn = document.getElementById('saveQueryBtn');\n        const clearSavedBtn = document.getElementById('clearSavedBtn');\n        const autoFillBtn = document.getElementById('autoFillVariablesBtn');\n        \n        if (saveQueryBtn) saveQueryBtn.addEventListener('click', () => this.saveCurrentQuery());\n        if (clearSavedBtn) clearSavedBtn.addEventListener('click', () => this.clearSavedQueries());\n        if (autoFillBtn) autoFillBtn.addEventListener('click', () => this.autoFillVariables());\n        \n        // Import scraping data button\n        const importBtn = document.createElement('button');\n        importBtn.className = 'action-btn';\n        importBtn.innerHTML = 'üìä ÿßÿ≥ÿ™Ÿäÿ±ÿßÿØ ÿ®ŸäÿßŸÜÿßÿ™ Web Scraping';\n        importBtn.addEventListener('click', () => this.importScrapingData());\n        \n        // Add to query interface when it's shown\n        setTimeout(() => {\n            const queryActions = document.querySelector('#queryInterface .results-actions');\n            if (queryActions && !document.getElementById('importScrapingBtn')) {\n                importBtn.id = 'importScrapingBtn';\n                queryActions.appendChild(importBtn);\n            }\n        }, 100);\n        \n        // Auto-save configuration\n        const endpointInput = document.getElementById('endpoint');\n        const secretKeyInput = document.getElementById('secretKey');\n        \n        endpointInput.addEventListener('input', () => this.saveConfig());\n        secretKeyInput.addEventListener('input', () => this.saveConfig());\n    }\n    \n    loadSavedConfig() {\n        const savedEndpoint = localStorage.getItem('hasura_endpoint');\n        const savedSecretKey = localStorage.getItem('hasura_secret_key');\n        \n        if (savedEndpoint) {\n            document.getElementById('endpoint').value = savedEndpoint;\n            this.endpoint = savedEndpoint;\n        }\n        \n        if (savedSecretKey) {\n            document.getElementById('secretKey').value = savedSecretKey;\n            this.secretKey = savedSecretKey;\n        }\n        \n        if (savedEndpoint && savedSecretKey) {\n            this.testConnection(false);\n        }\n    }\n    \n    saveConfig() {\n        const endpoint = document.getElementById('endpoint').value;\n        const secretKey = document.getElementById('secretKey').value;\n        \n        if (endpoint) localStorage.setItem('hasura_endpoint', endpoint);\n        if (secretKey) localStorage.setItem('hasura_secret_key', secretKey);\n        \n        this.endpoint = endpoint;\n        this.secretKey = secretKey;\n    }\n    \n    async handleConfigSubmit(event) {\n        event.preventDefault();\n        \n        const formData = new FormData(event.target);\n        this.endpoint = formData.get('endpoint');\n        this.secretKey = formData.get('secretKey');\n        \n        await this.testConnection(true);\n    }\n    \n    async testConnection(showNotification = true) {\n        const connectBtn = document.getElementById('connectBtn');\n        const statusDot = document.getElementById('dbStatusDot');\n        const statusText = document.getElementById('dbStatusText');\n        \n        try {\n            connectBtn.classList.add('loading');\n            statusDot.className = 'status-dot checking';\n            statusText.textContent = 'ÿ¨ÿßÿ±Ÿä ÿßŸÑÿßÿ™ÿµÿßŸÑ...';\n            \n            const response = await fetch(this.endpoint, {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                    'x-hasura-admin-secret': this.secretKey\n                },\n                body: JSON.stringify({\n                    query: `\n                        query {\n                            __schema {\n                                queryType {\n                                    name\n                                }\n                            }\n                        }\n                    `\n                })\n            });\n            \n            const data = await response.json();\n            \n            if (response.ok && data.data) {\n                statusDot.className = 'status-dot';\n                statusText.textContent = 'ŸÖÿ™ÿµŸÑ ‚úì';\n                this.isConnected = true;\n                \n                if (showNotification) {\n                    this.showNotification('ÿ™ŸÖ ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ®ŸÜÿ¨ÿßÿ≠', 'success');\n                }\n                \n                this.showQueryInterface();\n                this.loadSchema();\n                \n            } else {\n                throw new Error(data.error || 'ŸÅÿ¥ŸÑ ŸÅŸä ÿßŸÑÿßÿ™ÿµÿßŸÑ');\n            }\n            \n        } catch (error) {\n            statusDot.className = 'status-dot error';\n            statusText.textContent = 'ŸÅÿ¥ŸÑ ÿßŸÑÿßÿ™ÿµÿßŸÑ ‚úó';\n            this.isConnected = false;\n            \n            if (showNotification) {\n                this.showNotification(`ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑÿßÿ™ÿµÿßŸÑ: ${error.message}`, 'error');\n            }\n            \n        } finally {\n            connectBtn.classList.remove('loading');\n        }\n    }\n    \n    showQueryInterface() {\n        const queryInterface = document.getElementById('queryInterface');\n        queryInterface.style.display = 'block';\n        queryInterface.scrollIntoView({ behavior: 'smooth' });\n    }\n    \n    async loadSchema() {\n        try {\n            const response = await fetch(this.endpoint, {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                    'x-hasura-admin-secret': this.secretKey\n                },\n                body: JSON.stringify({\n                    query: `\n                        query {\n                            __schema {\n                                types {\n                                    name\n                                    kind\n                                    fields {\n                                        name\n                                        type {\n                                            name\n                                            kind\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    `\n                })\n            });\n            \n            const data = await response.json();\n            \n            if (data.data) {\n                this.displaySchema(data.data.__schema.types);\n            }\n            \n        } catch (error) {\n            console.error('Error loading schema:', error);\n        }\n    }\n    \n    displaySchema(types) {\n        const schemaContent = document.getElementById('schemaContent');\n        \n        // Filter out system types\n        const userTypes = types.filter(type => \n            !type.name.startsWith('__') && \n            type.kind === 'OBJECT' && \n            type.fields && \n            type.fields.length > 0\n        );\n        \n        let html = '<div class=\"schema-types\">';\n        \n        userTypes.forEach(type => {\n            html += `\n                <div class=\"example-query\">\n                    <h4>üìã ${type.name}</h4>\n                    <div style=\"margin-top: 0.5rem; font-size: 0.9em;\">\n            `;\n            \n            type.fields.forEach(field => {\n                const fieldType = field.type.name || field.type.kind;\n                html += `\n                    <div style=\"margin: 0.2rem 0; color: rgba(255,255,255,0.8);\">\n                        ‚Ä¢ <span style=\"color: #f093fb;\">${field.name}</span>: \n                        <span style=\"color: #4facfe;\">${fieldType}</span>\n                    </div>\n                `;\n            });\n            \n            html += `\n                    </div>\n                </div>\n            `;\n        });\n        \n        html += '</div>';\n        schemaContent.innerHTML = html;\n    }\n    \n    async handleQuerySubmit(event) {\n        event.preventDefault();\n        \n        if (!this.isConnected) {\n            this.showNotification('Ÿäÿ±ÿ¨Ÿâ ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ£ŸàŸÑÿßŸã', 'warning');\n            return;\n        }\n        \n        const formData = new FormData(event.target);\n        const query = formData.get('graphqlQuery');\n        const variablesText = formData.get('variables');\n        \n        let variables = {};\n        if (variablesText.trim()) {\n            try {\n                variables = JSON.parse(variablesText);\n                \n                // If we have scraping data, auto-fill the template variables\n                if (this.scrapingData && this.scrapingData.length > 0) {\n                    const firstItem = this.scrapingData[0].data;\n                    variables = this.generateVariablesFromData(variablesText, firstItem);\n                }\n                \n            } catch (error) {\n                this.showNotification('ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ŸÜÿ≥ŸäŸÇ Variables JSON', 'error');\n                return;\n            }\n        } else if (this.scrapingData && this.scrapingData.length > 0) {\n            // If no variables template but we have scraping data, use first item\n            variables = this.scrapingData[0].data;\n        }\n        \n        await this.executeQuery(query, variables);\n    }\n    \n    async executeQuery(query, variables = {}) {\n        const executeBtn = document.getElementById('executeBtn');\n        const queryResults = document.getElementById('queryResults');\n        const responseViewer = document.getElementById('responseViewer');\n        \n        try {\n            executeBtn.classList.add('loading');\n            \n            const response = await fetch(this.endpoint, {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                    'x-hasura-admin-secret': this.secretKey\n                },\n                body: JSON.stringify({\n                    query: query,\n                    variables: variables\n                })\n            });\n            \n            const data = await response.json();\n            \n            // Show results section\n            queryResults.style.display = 'block';\n            queryResults.scrollIntoView({ behavior: 'smooth' });\n            \n            // Display formatted response\n            const formattedResponse = this.formatJSON(data);\n            responseViewer.innerHTML = formattedResponse;\n            \n            if (data.errors) {\n                this.showNotification('Query executed with errors - check results', 'warning');\n            } else {\n                this.showNotification('Query executed successfully', 'success');\n            }\n            \n            // Store results for download\n            this.lastResults = data;\n            \n        } catch (error) {\n            this.showNotification(`ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ŸÜŸÅŸäÿ∞ Query: ${error.message}`, 'error');\n            \n        } finally {\n            executeBtn.classList.remove('loading');\n        }\n    }\n    \n    formatJSON(obj, indent = 0) {\n        const spaces = '  '.repeat(indent);\n        \n        if (obj === null) return '<span class=\"json-null\">null</span>';\n        if (typeof obj === 'boolean') return `<span class=\"json-boolean\">${obj}</span>`;\n        if (typeof obj === 'number') return `<span class=\"json-number\">${obj}</span>`;\n        if (typeof obj === 'string') return `<span class=\"json-string\">\"${obj}\"</span>`;\n        \n        if (Array.isArray(obj)) {\n            if (obj.length === 0) return '[]';\n            \n            let result = '[\\n';\n            obj.forEach((item, index) => {\n                result += `${spaces}  ${this.formatJSON(item, indent + 1)}`;\n                if (index < obj.length - 1) result += ',';\n                result += '\\n';\n            });\n            result += `${spaces}]`;\n            return result;\n        }\n        \n        if (typeof obj === 'object') {\n            const keys = Object.keys(obj);\n            if (keys.length === 0) return '{}';\n            \n            let result = '{\\n';\n            keys.forEach((key, index) => {\n                result += `${spaces}  <span class=\"json-key\">\"${key}\"</span>: ${this.formatJSON(obj[key], indent + 1)}`;\n                if (index < keys.length - 1) result += ',';\n                result += '\\n';\n            });\n            result += `${spaces}}`;\n            return result;\n        }\n        \n        return String(obj);\n    }\n    \n    downloadResults() {\n        if (!this.lastResults) {\n            this.showNotification('ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÜÿ™ÿßÿ¶ÿ¨ ŸÑŸÑÿ™ÿ≠ŸÖŸäŸÑ', 'warning');\n            return;\n        }\n        \n        const dataStr = JSON.stringify(this.lastResults, null, 2);\n        const dataBlob = new Blob([dataStr], { type: 'application/json' });\n        \n        const link = document.createElement('a');\n        link.href = URL.createObjectURL(dataBlob);\n        link.download = `query-results-${new Date().toISOString().slice(0, 10)}.json`;\n        \n        document.body.appendChild(link);\n        link.click();\n        document.body.removeChild(link);\n        \n        this.showNotification('ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ŸÜÿ¨ÿßÿ≠', 'success');\n    }\n    \n    clearResults() {\n        const queryResults = document.getElementById('queryResults');\n        const responseViewer = document.getElementById('responseViewer');\n        \n        queryResults.style.display = 'none';\n        responseViewer.innerHTML = '';\n        this.lastResults = null;\n        \n        this.showNotification('ÿ™ŸÖ ŸÖÿ≥ÿ≠ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨', 'success');\n    }\n    \n    checkForScrapingResults() {\n        const scrapingResults = localStorage.getItem('scraping_results');\n        if (scrapingResults) {\n            try {\n                const results = JSON.parse(scrapingResults);\n                this.scrapingData = results;\n                \n                // Show notification\n                setTimeout(() => {\n                    this.showNotification(`ÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ${results.length} ŸÜÿ™Ÿäÿ¨ÿ© ŸÖŸÜ Web Scraping - ÿßÿ∂ÿ∫ÿ∑ \"ÿßÿ≥ÿ™Ÿäÿ±ÿßÿØ ÿ®ŸäÿßŸÜÿßÿ™\" ŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖŸáÿß`, 'success');\n                }, 1000);\n                \n            } catch (error) {\n                console.error('Error parsing scraping results:', error);\n            }\n        }\n    }\n    \n    importScrapingData() {\n        if (!this.scrapingData || this.scrapingData.length === 0) {\n            this.showNotification('ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ®ŸäÿßŸÜÿßÿ™ Web Scraping ŸÑŸÑÿßÿ≥ÿ™Ÿäÿ±ÿßÿØ', 'warning');\n            return;\n        }\n        \n        // Generate the mutation query\n        const mutation = `mutation InsertData($content: String, $source_url: String, $title: String) {\n  insert_data(objects: {content: $content, source_url: $source_url, title: $title}) {\n    affected_rows\n    returning {\n      id\n      content\n      source_url\n      title\n      created_at\n    }\n  }\n}`;\n        \n        // Fill the query editor\n        document.getElementById('graphqlQuery').value = mutation;\n        \n        // Generate variables for the first item as example\n        const firstItem = this.scrapingData[0].data;\n        const variables = {\n            content: firstItem.content || \"\",\n            source_url: firstItem.source_url || \"\",\n            title: firstItem.title || \"\"\n        };\n        \n        document.getElementById('variables').value = JSON.stringify(variables, null, 2);\n        \n        // Switch to query tab\n        switchTab('query');\n        \n        // Scroll to query editor\n        document.getElementById('graphqlQuery').scrollIntoView({ behavior: 'smooth' });\n        \n        this.showNotification(`ÿ™ŸÖ ÿßÿ≥ÿ™Ÿäÿ±ÿßÿØ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™! ŸäŸÖŸÉŸÜŸÉ ÿßŸÑÿ¢ŸÜ ÿ•ÿ±ÿ≥ÿßŸÑ ${this.scrapingData.length} ÿπŸÜÿµÿ± ŸÑŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™`, 'success');\n        \n        // Show bulk insert option\n        this.showBulkInsertOption();\n    }\n    \n    showBulkInsertOption() {\n        // Create bulk insert button if it doesn't exist\n        if (!document.getElementById('bulkInsertBtn')) {\n            const bulkBtn = document.createElement('button');\n            bulkBtn.id = 'bulkInsertBtn';\n            bulkBtn.className = 'submit-btn';\n            bulkBtn.style.marginTop = '1rem';\n            bulkBtn.innerHTML = `\n                <span class=\"btn-icon\">üì¶</span>\n                <span class=\"btn-text\">ÿ•ÿ±ÿ≥ÿßŸÑ ÿ¨ŸÖŸäÿπ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ (${this.scrapingData.length} ÿπŸÜÿµÿ±)</span>\n                <div class=\"btn-loader\"></div>\n            `;\n            \n            bulkBtn.addEventListener('click', () => this.executeBulkInsert());\n            \n            // Add after the regular execute button\n            const executeBtn = document.getElementById('executeBtn');\n            executeBtn.parentNode.insertBefore(bulkBtn, executeBtn.nextSibling);\n        }\n    }\n    \n    async executeBulkInsert() {\n        if (!this.isConnected) {\n            this.showNotification('Ÿäÿ±ÿ¨Ÿâ ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ£ŸàŸÑÿßŸã', 'warning');\n            return;\n        }\n        \n        if (!this.scrapingData || this.scrapingData.length === 0) {\n            this.showNotification('ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ®ŸäÿßŸÜÿßÿ™ ŸÑŸÑÿ•ÿ±ÿ≥ÿßŸÑ', 'warning');\n            return;\n        }\n        \n        const bulkBtn = document.getElementById('bulkInsertBtn');\n        bulkBtn.classList.add('loading');\n        \n        // Get current query and variables\n        const queryTemplate = document.getElementById('graphqlQuery').value.trim();\n        const variablesTemplate = document.getElementById('variables').value.trim();\n        \n        if (!queryTemplate) {\n            this.showNotification('Ÿäÿ±ÿ¨Ÿâ ŸÉÿ™ÿßÿ®ÿ© Query ÿ£ŸàŸÑÿßŸã', 'warning');\n            bulkBtn.classList.remove('loading');\n            return;\n        }\n        \n        let successCount = 0;\n        let errorCount = 0;\n        \n        try {\n            // Process each item\n            for (let i = 0; i < this.scrapingData.length; i++) {\n                const item = this.scrapingData[i].data;\n                \n                try {\n                    // Generate variables from JSON data automatically\n                    const variables = this.generateVariablesFromData(variablesTemplate, item);\n                    \n                    const response = await fetch(this.endpoint, {\n                        method: 'POST',\n                        headers: {\n                            'Content-Type': 'application/json',\n                            'x-hasura-admin-secret': this.secretKey\n                        },\n                        body: JSON.stringify({\n                            query: queryTemplate,\n                            variables: variables\n                        })\n                    });\n                    \n                    const data = await response.json();\n                    \n                    if (data.errors) {\n                        errorCount++;\n                        console.error(`Error inserting item ${i + 1}:`, data.errors);\n                    } else {\n                        successCount++;\n                    }\n                    \n                } catch (itemError) {\n                    errorCount++;\n                    console.error(`Failed to insert item ${i + 1}:`, itemError);\n                }\n                \n                // Update button text with progress\n                const progressText = `ŸÖÿπÿßŸÑÿ¨ÿ© ${i + 1}/${this.scrapingData.length}...`;\n                bulkBtn.querySelector('.btn-text').textContent = progressText;\n            }\n            \n            // Show final results\n            if (successCount > 0) {\n                this.showNotification(`ÿ™ŸÖ ÿ•ÿ±ÿ≥ÿßŸÑ ${successCount} ÿπŸÜÿµÿ± ÿ®ŸÜÿ¨ÿßÿ≠${errorCount > 0 ? ` (${errorCount} ŸÅÿ¥ŸÑ)` : ''}`, 'success');\n                \n                // Clear scraping data after successful insert\n                localStorage.removeItem('scraping_results');\n                this.scrapingData = null;\n                \n                // Remove bulk insert button\n                bulkBtn.remove();\n                \n            } else {\n                this.showNotification(`ŸÅÿ¥ŸÑ ŸÅŸä ÿ•ÿ±ÿ≥ÿßŸÑ ÿ¨ŸÖŸäÿπ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ (${errorCount} ÿÆÿ∑ÿ£)`, 'error');\n            }\n            \n        } catch (error) {\n            this.showNotification(`ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑÿπŸÖŸÑŸäÿ©: ${error.message}`, 'error');\n            \n        } finally {\n            bulkBtn.classList.remove('loading');\n            bulkBtn.querySelector('.btn-text').textContent = `ÿ•ÿ±ÿ≥ÿßŸÑ ÿ¨ŸÖŸäÿπ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ (${this.scrapingData?.length || 0} ÿπŸÜÿµÿ±)`;\n        }\n    }\n    \n    generateVariablesFromData(variablesTemplate, jsonData) {\n        try {\n            // Parse the variables template\n            let variables = {};\n            \n            if (variablesTemplate) {\n                variables = JSON.parse(variablesTemplate);\n            } else {\n                // If no template, return the JSON data directly\n                return jsonData;\n            }\n            \n            // Only fill the variables that exist in the template\n            for (const templateKey in variables) {\n                // Look for exact match first\n                if (jsonData.hasOwnProperty(templateKey)) {\n                    variables[templateKey] = jsonData[templateKey] || \"\";\n                } else {\n                    // Try to find matching key (case insensitive)\n                    const dataKeys = Object.keys(jsonData);\n                    const matchingKey = dataKeys.find(k => \n                        k.toLowerCase() === templateKey.toLowerCase()\n                    );\n                    \n                    if (matchingKey) {\n                        variables[templateKey] = jsonData[matchingKey] || \"\";\n                    } else {\n                        // Keep original value if no match found\n                        variables[templateKey] = variables[templateKey] || \"\";\n                    }\n                }\n            }\n            \n            return variables;\n            \n        } catch (error) {\n            console.error('Error generating variables:', error);\n            // Fallback: return the JSON data directly\n            return jsonData;\n        }\n    }\n\n    saveCurrentQuery() {\n        const queryText = document.getElementById('graphqlQuery').value.trim();\n        const variablesText = document.getElementById('variables').value.trim();\n        const queryName = document.getElementById('queryName').value.trim() || `Query ${new Date().toLocaleString('ar-EG')}`;\n        \n        if (!queryText) {\n            this.showNotification('ŸÑÿß ŸäŸàÿ¨ÿØ query ŸÑÿ≠ŸÅÿ∏Ÿá', 'warning');\n            return;\n        }\n        \n        // Get saved queries from localStorage\n        let savedQueries = JSON.parse(localStorage.getItem('hasura_saved_queries') || '[]');\n        \n        // Create new query object\n        const newQuery = {\n            id: Date.now(),\n            name: queryName,\n            query: queryText,\n            variables: variablesText,\n            createdAt: new Date().toISOString()\n        };\n        \n        // Add to saved queries\n        savedQueries.unshift(newQuery); // Add to beginning\n        \n        // Keep only last 20 queries\n        if (savedQueries.length > 20) {\n            savedQueries = savedQueries.slice(0, 20);\n        }\n        \n        // Save to localStorage\n        localStorage.setItem('hasura_saved_queries', JSON.stringify(savedQueries));\n        \n        // Refresh the display\n        this.loadSavedQueries();\n        \n        // Clear query name field\n        document.getElementById('queryName').value = '';\n        \n        this.showNotification(`ÿ™ŸÖ ÿ≠ŸÅÿ∏ Query: ${queryName}`, 'success');\n    }\n    \n    loadSavedQueries() {\n        const savedQueries = JSON.parse(localStorage.getItem('hasura_saved_queries') || '[]');\n        const container = document.getElementById('savedQueriesList');\n        \n        if (savedQueries.length === 0) {\n            container.innerHTML = '<p style=\"color: rgba(255,255,255,0.6); font-style: italic;\">ŸÑÿß ÿ™Ÿàÿ¨ÿØ queries ŸÖÿ≠ŸÅŸàÿ∏ÿ©</p>';\n            return;\n        }\n        \n        let html = '';\n        savedQueries.forEach(savedQuery => {\n            const date = new Date(savedQuery.createdAt).toLocaleDateString('ar-EG');\n            const time = new Date(savedQuery.createdAt).toLocaleTimeString('ar-EG', {\n                hour: '2-digit',\n                minute: '2-digit'\n            });\n            \n            html += `\n                <div class=\"example-query\" style=\"position: relative;\">\n                    <div style=\"display: flex; justify-content: space-between; align-items: flex-start; margin-bottom: 0.5rem;\">\n                        <h4 style=\"margin: 0; color: #f093fb;\">üìÑ ${savedQuery.name}</h4>\n                        <div style=\"display: flex; gap: 0.5rem;\">\n                            <button \n                                class=\"action-btn\" \n                                style=\"padding: 0.25rem 0.5rem; font-size: 11px; background: rgba(255,255,255,0.1);\"\n                                onclick=\"window.databaseManager.loadSavedQuery(${savedQuery.id})\"\n                            >\n                                üìÇ ÿ™ÿ≠ŸÖŸäŸÑ\n                            </button>\n                            <button \n                                class=\"action-btn\" \n                                style=\"padding: 0.25rem 0.5rem; font-size: 11px; background: rgba(255,0,0,0.2);\"\n                                onclick=\"window.databaseManager.deleteSavedQuery(${savedQuery.id})\"\n                            >\n                                üóëÔ∏è ÿ≠ÿ∞ŸÅ\n                            </button>\n                        </div>\n                    </div>\n                    <div style=\"font-size: 11px; color: rgba(255,255,255,0.5); margin-bottom: 0.5rem;\">\n                        ${date} - ${time}\n                    </div>\n                    <pre style=\"font-size: 11px; max-height: 100px; overflow-y: auto; background: rgba(0,0,0,0.2); padding: 0.5rem; border-radius: 4px; margin: 0;\">${savedQuery.query}</pre>\n                    ${savedQuery.variables ? `<div style=\"margin-top: 0.5rem;\"><strong style=\"font-size: 11px; color: #4facfe;\">Variables:</strong><pre style=\"font-size: 10px; background: rgba(0,0,0,0.2); padding: 0.5rem; border-radius: 4px; margin: 0.25rem 0 0 0; max-height: 60px; overflow-y: auto;\">${savedQuery.variables}</pre></div>` : ''}\n                </div>\n            `;\n        });\n        \n        container.innerHTML = html;\n    }\n    \n    loadSavedQuery(queryId) {\n        const savedQueries = JSON.parse(localStorage.getItem('hasura_saved_queries') || '[]');\n        const query = savedQueries.find(q => q.id === queryId);\n        \n        if (!query) {\n            this.showNotification('Query ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØ', 'error');\n            return;\n        }\n        \n        // Load into editors\n        document.getElementById('graphqlQuery').value = query.query;\n        document.getElementById('variables').value = query.variables || '';\n        document.getElementById('queryName').value = query.name;\n        \n        // Scroll to query editor\n        document.getElementById('graphqlQuery').scrollIntoView({ behavior: 'smooth' });\n        \n        this.showNotification(`ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ Query: ${query.name}`, 'success');\n    }\n    \n    deleteSavedQuery(queryId) {\n        if (!confirm('ŸáŸÑ ÿ£ŸÜÿ™ ŸÖÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿ≠ÿ∞ŸÅ Ÿáÿ∞ÿß Queryÿü')) {\n            return;\n        }\n        \n        let savedQueries = JSON.parse(localStorage.getItem('hasura_saved_queries') || '[]');\n        const queryName = savedQueries.find(q => q.id === queryId)?.name || 'Query';\n        \n        savedQueries = savedQueries.filter(q => q.id !== queryId);\n        localStorage.setItem('hasura_saved_queries', JSON.stringify(savedQueries));\n        \n        this.loadSavedQueries();\n        this.showNotification(`ÿ™ŸÖ ÿ≠ÿ∞ŸÅ Query: ${queryName}`, 'success');\n    }\n    \n    clearSavedQueries() {\n        if (!confirm('ŸáŸÑ ÿ£ŸÜÿ™ ŸÖÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿ≠ÿ∞ŸÅ ÿ¨ŸÖŸäÿπ Queries ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ÿ©ÿü')) {\n            return;\n        }\n        \n        localStorage.removeItem('hasura_saved_queries');\n        this.loadSavedQueries();\n        this.showNotification('ÿ™ŸÖ ÿ≠ÿ∞ŸÅ ÿ¨ŸÖŸäÿπ Queries ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ÿ©', 'success');\n    }\n    \n    autoFillVariables() {\n        if (!this.scrapingData || this.scrapingData.length === 0) {\n            this.showNotification('ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ®ŸäÿßŸÜÿßÿ™ Web Scraping ŸÑŸÑŸÖŸÑÿ° ÿßŸÑÿ™ŸÑŸÇÿßÿ¶Ÿä', 'warning');\n            return;\n        }\n        \n        const variablesTextarea = document.getElementById('variables');\n        const currentVariables = variablesTextarea.value.trim();\n        \n        // Get first item data as sample\n        const sampleData = this.scrapingData[0].data;\n        \n        let newVariables = {};\n        \n        try {\n            // If there are existing variables, parse them first\n            if (currentVariables) {\n                newVariables = JSON.parse(currentVariables);\n            }\n            \n            // Auto-fill only the variables that already exist in template\n            Object.keys(newVariables).forEach(templateKey => {\n                // Look for exact match in sample data\n                if (sampleData.hasOwnProperty(templateKey)) {\n                    newVariables[templateKey] = sampleData[templateKey] || \"\";\n                } else {\n                    // Try case insensitive match\n                    const sampleKeys = Object.keys(sampleData);\n                    const matchingKey = sampleKeys.find(k => \n                        k.toLowerCase() === templateKey.toLowerCase()\n                    );\n                    \n                    if (matchingKey) {\n                        newVariables[templateKey] = sampleData[matchingKey] || \"\";\n                    }\n                }\n            });\n            \n            // Update textarea with formatted JSON\n            variablesTextarea.value = JSON.stringify(newVariables, null, 2);\n            \n            this.showNotification(`ÿ™ŸÖ ŸÖŸÑÿ° ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿ™ŸÑŸÇÿßÿ¶ŸäÿßŸã ŸÖŸÜ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿπŸÜÿµÿ± ÿßŸÑÿ£ŸàŸÑ (${Object.keys(newVariables).length} ŸÖÿ™ÿ∫Ÿäÿ±)`, 'success');\n            \n        } catch (error) {\n            // If parsing fails, create fresh variables from sample data\n            variablesTextarea.value = JSON.stringify(sampleData, null, 2);\n            this.showNotification(`ÿ™ŸÖ ÿ•ŸÜÿ¥ÿßÿ° ŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿ¨ÿØŸäÿØÿ© ŸÖŸÜ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ (${Object.keys(sampleData).length} ŸÖÿ™ÿ∫Ÿäÿ±)`, 'success');\n        }\n    }\n\n    showNotification(message, type = 'info') {\n        const container = document.getElementById('notificationContainer');\n        \n        const notification = document.createElement('div');\n        notification.className = `notification ${type}`;\n        notification.textContent = message;\n        \n        container.appendChild(notification);\n        \n        // Auto remove after 5 seconds\n        setTimeout(() => {\n            notification.style.animation = 'slideOutRight 0.3s ease forwards';\n            setTimeout(() => {\n                if (notification.parentNode) {\n                    notification.parentNode.removeChild(notification);\n                }\n            }, 300);\n        }, 5000);\n        \n        // Click to dismiss\n        notification.addEventListener('click', () => {\n            notification.remove();\n        });\n    }\n}\n\n// Tab switching function\nfunction switchTab(tabName) {\n    // Hide all tab contents\n    const tabContents = document.querySelectorAll('.tab-content');\n    tabContents.forEach(content => content.classList.remove('active'));\n    \n    // Remove active class from all tab buttons\n    const tabButtons = document.querySelectorAll('.tab-button');\n    tabButtons.forEach(button => button.classList.remove('active'));\n    \n    // Show selected tab content\n    document.getElementById(tabName + 'Tab').classList.add('active');\n    \n    // Add active class to clicked button\n    event.target.classList.add('active');\n}\n\n// Use example query\nfunction useExample(element) {\n    const queryText = element.querySelector('pre').textContent;\n    document.getElementById('graphqlQuery').value = queryText.trim();\n    \n    // Switch to query tab\n    switchTab('query');\n    document.querySelector('.tab-button').classList.remove('active');\n    document.querySelector('.tab-button').classList.add('active');\n    \n    // Scroll to query editor\n    document.getElementById('graphqlQuery').scrollIntoView({ behavior: 'smooth' });\n}\n\n// Initialize the application\ndocument.addEventListener('DOMContentLoaded', () => {\n    window.databaseManager = new DatabaseManager();\n});\n\n// Export for potential external use\nif (typeof module !== 'undefined' && module.exports) {\n    module.exports = DatabaseManager;\n}","size_bytes":34089},"static/script.js":{"content":"// Modern Web Scraping API UI - Elegant & Calm Design\nclass WebScrapingUI {\n    constructor() {\n        this.apiBaseUrl = window.location.origin;\n        this.currentRequest = null;\n        this.results = [];\n        this.eventSource = null;\n        \n        this.init();\n    }\n    \n    init() {\n        this.bindEvents();\n        this.checkApiStatus();\n        this.setupFormValidation();\n        this.handleModeChange(); // Initialize mode display\n        this.setupHoverEffects();\n    }\n    \n    setupHoverEffects() {\n        // Add subtle hover effects for interactive elements\n        const interactiveElements = document.querySelectorAll('button, .radio-option, .result-card, .action-btn');\n        interactiveElements.forEach(el => {\n            el.addEventListener('mouseenter', () => {\n                el.style.transform = 'translateY(-2px)';\n                el.style.transition = 'all 0.3s ease';\n            });\n            \n            el.addEventListener('mouseleave', () => {\n                el.style.transform = 'translateY(0)';\n            });\n        });\n    }\n    \n    bindEvents() {\n        // Form submission\n        const form = document.getElementById('scrapeForm');\n        form.addEventListener('submit', (e) => this.handleFormSubmit(e));\n        \n        // Action buttons\n        const downloadBtn = document.getElementById('downloadBtn');\n        const sendToDbBtn = document.getElementById('sendToDbBtn');\n        const clearBtn = document.getElementById('clearBtn');\n        \n        downloadBtn.addEventListener('click', () => this.downloadResults());\n        sendToDbBtn.addEventListener('click', () => this.sendToDatabase());\n        clearBtn.addEventListener('click', () => this.clearResults());\n        \n        // Input validation\n        const urlInput = document.getElementById('url');\n        urlInput.addEventListener('input', () => this.validateUrl(urlInput.value));\n        \n        // Real-time form updates\n        const maxPagesInput = document.getElementById('maxPages');\n        const timeoutInput = document.getElementById('timeout');\n        \n        maxPagesInput.addEventListener('input', () => this.updateFormHints());\n        timeoutInput.addEventListener('input', () => this.updateFormHints());\n        \n        // Scraping mode radio buttons\n        const modeRadios = document.querySelectorAll('input[name=\"scrapingMode\"]');\n        modeRadios.forEach(radio => {\n            radio.addEventListener('change', () => this.handleModeChange());\n        });\n    }\n    \n    async checkApiStatus() {\n        const statusDot = document.getElementById('statusDot');\n        const statusText = document.getElementById('statusText');\n        \n        try {\n            statusDot.className = 'status-dot checking';\n            statusText.textContent = 'ÿ¨ÿßÿ±Ÿä ÿßŸÑÿ™ÿ≠ŸÇŸÇ...';\n            \n            const response = await fetch(`${this.apiBaseUrl}/health`);\n            const data = await response.json();\n            \n            if (response.ok && data.status === 'healthy') {\n                statusDot.className = 'status-dot';\n                statusText.textContent = 'ŸÖÿ™ÿµŸÑ ‚úì';\n                this.showNotification('API ŸÖÿ™ÿµŸÑ Ÿàÿ¨ÿßŸáÿ≤ ŸÑŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ', 'success');\n            } else {\n                throw new Error('API ÿ∫Ÿäÿ± ŸÖÿ™ÿßÿ≠');\n            }\n        } catch (error) {\n            statusDot.className = 'status-dot error';\n            statusText.textContent = 'ÿ∫Ÿäÿ± ŸÖÿ™ÿµŸÑ ‚úó';\n            this.showNotification('ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ŸÄ API', 'error');\n        }\n    }\n    \n    setupFormValidation() {\n        const form = document.getElementById('scrapeForm');\n        const inputs = form.querySelectorAll('input[required]');\n        \n        inputs.forEach(input => {\n            input.addEventListener('blur', () => this.validateField(input));\n            input.addEventListener('input', () => this.clearFieldError(input));\n        });\n    }\n    \n    validateField(field) {\n        const value = field.value.trim();\n        \n        if (field.type === 'url' && value) {\n            return this.validateUrl(value);\n        }\n        \n        if (field.required && !value) {\n            this.showFieldError(field, 'Ÿáÿ∞ÿß ÿßŸÑÿ≠ŸÇŸÑ ŸÖÿ∑ŸÑŸàÿ®');\n            return false;\n        }\n        \n        return true;\n    }\n    \n    validateUrl(url) {\n        const urlInput = document.getElementById('url');\n        \n        if (!url) {\n            this.clearFieldError(urlInput);\n            return false;\n        }\n        \n        try {\n            const urlObj = new URL(url);\n            if (!['http:', 'https:'].includes(urlObj.protocol)) {\n                this.showFieldError(urlInput, 'ÿßŸÑÿ±ÿßÿ®ÿ∑ Ÿäÿ¨ÿ® ÿ£ŸÜ Ÿäÿ®ÿØÿ£ ÿ®ŸÄ http:// ÿ£Ÿà https://');\n                return false;\n            }\n            \n            this.clearFieldError(urlInput);\n            return true;\n        } catch (error) {\n            this.showFieldError(urlInput, 'ÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑÿ±ÿßÿ®ÿ∑ ÿ∫Ÿäÿ± ÿµÿ≠Ÿäÿ≠');\n            return false;\n        }\n    }\n    \n    showFieldError(field, message) {\n        this.clearFieldError(field);\n        \n        field.style.borderColor = '#ff416c';\n        field.style.boxShadow = '0 0 10px rgba(255, 65, 108, 0.3)';\n        \n        const errorDiv = document.createElement('div');\n        errorDiv.className = 'field-error';\n        errorDiv.textContent = message;\n        errorDiv.style.cssText = `\n            color: #ff416c;\n            font-size: 0.875rem;\n            margin-top: 0.5rem;\n            animation: slideInUp 0.3s ease;\n        `;\n        \n        field.parentNode.appendChild(errorDiv);\n    }\n    \n    clearFieldError(field) {\n        field.style.borderColor = '';\n        field.style.boxShadow = '';\n        \n        const errorDiv = field.parentNode.querySelector('.field-error');\n        if (errorDiv) {\n            errorDiv.remove();\n        }\n    }\n    \n    updateFormHints() {\n        const maxPages = document.getElementById('maxPages').value;\n        const timeout = document.getElementById('timeout').value;\n        \n        // Update estimated time\n        const estimatedTime = Math.ceil((maxPages * timeout) / 10);\n        \n        // You could add a hint element to show estimated completion time\n        // This is a placeholder for future enhancement\n    }\n    \n    handleModeChange() {\n        const selectedMode = document.querySelector('input[name=\"scrapingMode\"]:checked').value;\n        const maxPagesGroup = document.getElementById('maxPagesGroup');\n        const maxPagesInput = document.getElementById('maxPages');\n        const submitBtn = document.getElementById('submitBtn');\n        const btnText = submitBtn.querySelector('.btn-text');\n        \n        if (selectedMode === 'single') {\n            maxPagesGroup.style.opacity = '0.5';\n            maxPagesInput.disabled = true;\n            btnText.textContent = 'üéØ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿµŸÅÿ≠ÿ© Ÿàÿßÿ≠ÿØÿ©';\n        } else if (selectedMode === 'stream') {\n            maxPagesGroup.style.opacity = '1';\n            maxPagesInput.disabled = false;\n            btnText.textContent = 'üì° ÿ®ÿ´ ŸÖÿ®ÿßÿ¥ÿ± ŸÑŸÑŸÜÿ™ÿßÿ¶ÿ¨';\n        } else if (selectedMode === 'unlimited') {\n            maxPagesGroup.style.opacity = '0.5';\n            maxPagesInput.disabled = true;\n            btnText.textContent = 'üöÄ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ŸÉÿßŸÖŸÑ ŸÑŸÑŸÖŸàŸÇÿπ';\n        } else {\n            maxPagesGroup.style.opacity = '1';\n            maxPagesInput.disabled = false;\n            btnText.textContent = 'ÿ®ÿØÿ° ÿßŸÑÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨';\n        }\n    }\n    \n    async handleFormSubmit(event) {\n        event.preventDefault();\n        \n        const form = event.target;\n        const formData = new FormData(form);\n        \n        // Validate form\n        if (!this.validateForm(form)) {\n            this.showNotification('Ÿäÿ±ÿ¨Ÿâ ÿ™ÿµÿ≠Ÿäÿ≠ ÿßŸÑÿ£ÿÆÿ∑ÿßÿ° ŸÅŸä ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨', 'error');\n            return;\n        }\n        \n        const url = formData.get('url');\n        const selectedMode = document.querySelector('input[name=\"scrapingMode\"]:checked').value;\n        let maxPages;\n        if (selectedMode === 'unlimited') {\n            maxPages = 999999;\n        } else if (selectedMode === 'single') {\n            maxPages = 1; // Will be ignored for single page endpoint\n        } else if (selectedMode === 'stream') {\n            maxPages = parseInt(formData.get('maxPages'));\n        } else {\n            maxPages = parseInt(formData.get('maxPages'));\n        }\n        const timeout = parseInt(formData.get('timeout'));\n        \n        if (selectedMode === 'unlimited') {\n            const confirmed = confirm('‚ö†Ô∏è ÿ™ÿ≠ÿ∞Ÿäÿ±: ÿ≥ÿ™ŸÇŸàŸÖ ÿ®ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿ¨ŸÖŸäÿπ ÿµŸÅÿ≠ÿßÿ™ ÿßŸÑŸÖŸàŸÇÿπ ÿ®ÿØŸàŸÜ ÿ≠ÿØ ÿ£ŸÇÿµŸâ.\\nŸÇÿØ Ÿäÿ≥ÿ™ÿ∫ÿ±ŸÇ Ÿáÿ∞ÿß ŸàŸÇÿ™ÿßŸã ÿ∑ŸàŸäŸÑÿßŸã ÿ¨ÿØÿßŸã ŸàŸäÿ≥ÿ™ŸáŸÑŸÉ ŸÖŸàÿßÿ±ÿØ ŸÉÿ´Ÿäÿ±ÿ©.\\nŸáŸÑ ÿ™ÿ±ŸäÿØ ÿßŸÑŸÖÿ™ÿßÿ®ÿπÿ©ÿü');\n            if (!confirmed) {\n                return;\n            }\n        }\n        \n        await this.startScraping(url, maxPages, timeout, selectedMode);\n    }\n    \n    validateForm(form) {\n        const inputs = form.querySelectorAll('input[required]');\n        let isValid = true;\n        \n        inputs.forEach(input => {\n            if (!this.validateField(input)) {\n                isValid = false;\n            }\n        });\n        \n        return isValid;\n    }\n    \n    async startScraping(url, maxPages, timeout, mode = 'limited') {\n        const submitBtn = document.getElementById('submitBtn');\n        const progressSection = document.getElementById('progressSection');\n        const resultsSection = document.getElementById('resultsSection');\n        \n        try {\n            // Update UI to loading state\n            submitBtn.classList.add('loading');\n            this.showProgress();\n            this.hideResults();\n            \n            // Clear previous results\n            this.results = [];\n            \n            // Show progress section\n            progressSection.style.display = 'block';\n            progressSection.scrollIntoView({ behavior: 'smooth' });\n            \n            let requestUrl, data;\n            \n            if (mode === 'single') {\n                // Single page scraping\n                this.updateProgress(50, 'ÿ¨ÿßÿ±Ÿä ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑÿµŸÅÿ≠ÿ©...');\n                requestUrl = `${this.apiBaseUrl}/scrape-single?url=${encodeURIComponent(url)}&timeout=${timeout}`;\n                \n                const response = await fetch(requestUrl, {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json',\n                    }\n                });\n                \n                if (!response.ok) {\n                    const errorData = await response.json();\n                    throw new Error(errorData.detail || 'ÿÆÿ∑ÿ£ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ∑ŸÑÿ®');\n                }\n                \n                const singleData = await response.json();\n                data = [singleData]; // Convert to array format\n            } else if (mode === 'unlimited') {\n                // Unlimited streaming scraping\n                await this.startUnlimitedStreamingScraping(url, timeout);\n                return; // Exit early as streaming handles everything\n            } else if (mode === 'stream') {\n                // Real-time streaming scraping with same duplicate prevention\n                await this.startStreamingScraping(url, maxPages, timeout);\n                return; // Exit early as streaming handles everything\n            } else {\n                // Limited scraping\n                this.simulateProgress(maxPages);\n                requestUrl = `${this.apiBaseUrl}/scrape-pages?url=${encodeURIComponent(url)}&max_pages=${maxPages}&timeout=${timeout}`;\n                \n                const response = await fetch(requestUrl, {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json',\n                    }\n                });\n                \n                if (!response.ok) {\n                    const errorData = await response.json();\n                    throw new Error(errorData.detail || 'ÿÆÿ∑ÿ£ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ∑ŸÑÿ®');\n                }\n                \n                data = await response.json();\n            }\n            \n            this.results = data;\n            \n            // Update progress to 100%\n            let modeText;\n            if (mode === 'single') {\n                modeText = 'ÿµŸÅÿ≠ÿ© Ÿàÿßÿ≠ÿØÿ©';\n            } else if (mode === 'unlimited') {\n                modeText = 'ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ŸÉÿßŸÖŸÑ';\n            } else {\n                modeText = 'ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ŸÖÿ≠ÿØŸàÿØ';\n            }\n            \n            this.updateProgress(100, `ÿ™ŸÖ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ° - ${data.length} ÿµŸÅÿ≠ÿ© (${modeText})`);\n            \n            // Show results\n            this.displayResults(data);\n            this.showNotification(`ÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ${data.length} ÿµŸÅÿ≠ÿ© ÿ®ŸÜÿ¨ÿßÿ≠ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ${modeText}`, 'success');\n            \n        } catch (error) {\n            console.error('Scraping error:', error);\n            this.showNotification(`ÿÆÿ∑ÿ£: ${error.message}`, 'error');\n            this.updateProgress(0, 'ŸÅÿ¥ŸÑ ŸÅŸä ÿßŸÑŸÖÿπÿßŸÑÿ¨ÿ©');\n        } finally {\n            submitBtn.classList.remove('loading');\n        }\n    }\n    \n    showProgress() {\n        const progressSection = document.getElementById('progressSection');\n        progressSection.style.display = 'block';\n        this.updateProgress(0, 'ÿ®ÿØÿ° ÿßŸÑŸÖÿπÿßŸÑÿ¨ÿ©...');\n    }\n    \n    hideProgress() {\n        const progressSection = document.getElementById('progressSection');\n        progressSection.style.display = 'none';\n    }\n    \n    updateProgress(percentage, text) {\n        const progressFill = document.querySelector('.progress-fill');\n        const progressText = document.getElementById('progressText');\n        \n        progressFill.style.width = `${percentage}%`;\n        progressText.textContent = text;\n    }\n    \n    simulateProgress(maxPages) {\n        let progress = 0;\n        const increment = 90 / maxPages; // Leave 10% for final processing\n        \n        const interval = setInterval(() => {\n            progress += increment;\n            if (progress >= 90) {\n                clearInterval(interval);\n                this.updateProgress(90, 'ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨...');\n                return;\n            }\n            \n            const currentPage = Math.floor(progress / increment);\n            this.updateProgress(progress, `ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿµŸÅÿ≠ÿ© ${currentPage} ŸÖŸÜ ${maxPages}...`);\n        }, 1000);\n    }\n    \n    simulateUnlimitedProgress() {\n        let progress = 0;\n        let pageCount = 0;\n        \n        const interval = setInterval(() => {\n            progress += 2; // Slower increment for unlimited mode\n            pageCount += Math.floor(Math.random() * 5) + 1; // Random pages discovered\n            \n            if (progress >= 90) {\n                clearInterval(interval);\n                this.updateProgress(90, `ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ©... (${pageCount}+ ÿµŸÅÿ≠ÿ©)`);\n                return;\n            }\n            \n            this.updateProgress(progress, `üîç ÿßŸÉÿ™ÿ¥ÿßŸÅ Ÿàÿ¨ŸÑÿ® ÿßŸÑÿµŸÅÿ≠ÿßÿ™... (${pageCount} ÿµŸÅÿ≠ÿ© ÿ™ŸÖÿ™ ŸÖÿπÿßŸÑÿ¨ÿ™Ÿáÿß)`);\n        }, 1500);\n    }\n    \n    displayResults(data) {\n        const resultsSection = document.getElementById('resultsSection');\n        const resultsGrid = document.getElementById('resultsGrid');\n        \n        // Clear previous results\n        resultsGrid.innerHTML = '';\n        \n        // Create result cards\n        data.forEach((item, index) => {\n            const card = this.createResultCard(item.data, index);\n            resultsGrid.appendChild(card);\n        });\n        \n        // Show results section\n        resultsSection.style.display = 'block';\n        resultsSection.scrollIntoView({ behavior: 'smooth' });\n    }\n    \n    createResultCard(data, index) {\n        const card = document.createElement('div');\n        card.className = 'result-card';\n        card.style.animationDelay = `${index * 0.1}s`;\n        \n        const title = data.title || 'ÿ®ÿØŸàŸÜ ÿπŸÜŸàÿßŸÜ';\n        const content = data.content || 'ŸÑÿß ŸäŸàÿ¨ÿØ ŸÖÿ≠ÿ™ŸàŸâ';\n        const url = data.source_url || '';\n        const createdAt = new Date(data.created_at).toLocaleString('ar-EG');\n        const wordCount = content.split(' ').length;\n        \n        card.innerHTML = `\n            <h4 title=\"${title}\">${title}</h4>\n            <div class=\"url\">${url}</div>\n            <div class=\"content\">${content}</div>\n            <div class=\"meta\">\n                <span>üìÖ ${createdAt}</span>\n                <span>üìù ${wordCount} ŸÉŸÑŸÖÿ©</span>\n                <span>üÜî ${data.id.slice(0, 8)}...</span>\n            </div>\n        `;\n        \n        // Add click event to show full content\n        card.addEventListener('click', () => this.showFullContent(data));\n        \n        return card;\n    }\n    \n    showFullContent(data) {\n        // Create modal or expanded view\n        const modal = document.createElement('div');\n        modal.className = 'content-modal';\n        modal.style.cssText = `\n            position: fixed;\n            top: 0;\n            left: 0;\n            width: 100%;\n            height: 100%;\n            background: rgba(0, 0, 0, 0.8);\n            backdrop-filter: blur(10px);\n            z-index: 1000;\n            display: flex;\n            align-items: center;\n            justify-content: center;\n            padding: 2rem;\n            animation: fadeIn 0.3s ease;\n        `;\n        \n        const content = document.createElement('div');\n        content.style.cssText = `\n            background: rgba(255, 255, 255, 0.1);\n            backdrop-filter: blur(20px);\n            border: 1px solid rgba(255, 255, 255, 0.2);\n            border-radius: 16px;\n            padding: 2rem;\n            max-width: 800px;\n            max-height: 80vh;\n            overflow-y: auto;\n            color: white;\n        `;\n        \n        content.innerHTML = `\n            <div style=\"display: flex; justify-content: space-between; align-items: center; margin-bottom: 1rem;\">\n                <h3>${data.title || 'ÿ®ÿØŸàŸÜ ÿπŸÜŸàÿßŸÜ'}</h3>\n                <button onclick=\"this.closest('.content-modal').remove()\" style=\"\n                    background: rgba(255, 255, 255, 0.2);\n                    border: none;\n                    border-radius: 8px;\n                    color: white;\n                    padding: 0.5rem 1rem;\n                    cursor: pointer;\n                \">‚úï</button>\n            </div>\n            <div style=\"margin-bottom: 1rem; font-size: 0.9rem; color: #a0a0a0;\">\n                üîó ${data.source_url}\n            </div>\n            <div style=\"line-height: 1.6;\">\n                ${data.content}\n            </div>\n            <div style=\"margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255, 255, 255, 0.1); font-size: 0.8rem; color: #a0a0a0;\">\n                üìÖ ${new Date(data.created_at).toLocaleString('ar-EG')} | üÜî ${data.id}\n            </div>\n        `;\n        \n        modal.appendChild(content);\n        document.body.appendChild(modal);\n        \n        // Close on outside click\n        modal.addEventListener('click', (e) => {\n            if (e.target === modal) {\n                modal.remove();\n            }\n        });\n    }\n    \n    async startStreamingScraping(url, maxPages, timeout) {\n        const submitBtn = document.getElementById('submitBtn');\n        const progressSection = document.getElementById('progressSection');\n        const resultsSection = document.getElementById('resultsSection');\n        \n        try {\n            // Update UI to loading state\n            submitBtn.classList.add('loading');\n            this.showProgress();\n            this.hideResults();\n            \n            // Clear previous results\n            this.results = [];\n            \n            // Show progress section\n            progressSection.style.display = 'block';\n            progressSection.scrollIntoView({ behavior: 'smooth' });\n            \n            // Prepare streaming request\n            const streamUrl = `${this.apiBaseUrl}/scrape-stream`;\n            const requestBody = {\n                url: url,\n                max_pages: maxPages,\n                timeout: timeout\n            };\n            \n            // Start streaming with fetch and response.body.getReader()\n            const response = await fetch(streamUrl, {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                },\n                body: JSON.stringify(requestBody)\n            });\n            \n            if (!response.ok) {\n                throw new Error(`HTTP error! status: ${response.status}`);\n            }\n            \n            // Process streaming response\n            const reader = response.body.getReader();\n            const decoder = new TextDecoder();\n            let buffer = '';\n            \n            while (true) {\n                const { done, value } = await reader.read();\n                \n                if (done) break;\n                \n                // Decode the chunk and add to buffer\n                buffer += decoder.decode(value, { stream: true });\n                \n                // Process complete lines from buffer\n                const lines = buffer.split('\\n');\n                buffer = lines.pop(); // Keep incomplete line in buffer\n                \n                for (const line of lines) {\n                    if (line.trim() && line.startsWith('data: ')) {\n                        try {\n                            const jsonStr = line.substring(6); // Remove \"data: \" prefix\n                            const data = JSON.parse(jsonStr);\n                            \n                            switch (data.type) {\n                                case 'start':\n                                    this.updateProgress(0, data.message);\n                                    this.showNotification('ÿ®ÿØÿ° ÿßŸÑÿ®ÿ´ ÿßŸÑŸÖÿ®ÿßÿ¥ÿ± ŸÑŸÑŸÜÿ™ÿßÿ¶ÿ¨', 'info');\n                                    break;\n                                    \n                                case 'page':\n                                    // Add new page to results immediately\n                                    this.results.push({ data: data.data });\n                                    \n                                    // Update progress\n                                    const percentage = data.progress ? data.progress.percentage : 0;\n                                    const current = data.progress ? data.progress.current : this.results.length;\n                                    const total = data.progress ? data.progress.total : maxPages;\n                                    \n                                    this.updateProgress(percentage, `ÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ${current} ŸÖŸÜ ${total} ÿµŸÅÿ≠ÿßÿ™...`);\n                                    \n                                    // Update results display in real-time\n                                    this.showResults();\n                                    this.displayStreamingResults();\n                                    \n                                    break;\n                                    \n                                case 'complete':\n                                    this.updateProgress(100, data.message);\n                                    this.showNotification(`ÿ™ŸÖ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°! ÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ${data.total_pages} ÿµŸÅÿ≠ÿ©`, 'success');\n                                    \n                                    // Final results display\n                                    this.showResults();\n                                    this.displayResults(this.results);\n                                    \n                                    return; // Exit the function\n                                    \n                                case 'error':\n                                    throw new Error(data.message);\n                            }\n                        } catch (parseError) {\n                            console.error('Error parsing stream data:', parseError);\n                        }\n                    }\n                }\n            }\n            \n        } catch (error) {\n            this.showNotification(error.message || 'ÿÆÿ∑ÿ£ ŸÅŸä ÿ®ÿØÿ° ÿßŸÑÿ®ÿ´ ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±', 'error');\n            console.error('Streaming error:', error);\n        } finally {\n            submitBtn.classList.remove('loading');\n        }\n    }\n    \n    displayStreamingResults() {\n        // Update the results display with current results\n        const resultsGrid = document.getElementById('resultsGrid');\n        if (!resultsGrid) return;\n        \n        // Clear and rebuild results to show latest data\n        resultsGrid.innerHTML = '';\n        \n        this.results.forEach((result, index) => {\n            const resultCard = this.createResultCard(result.data, index);\n            resultsGrid.appendChild(resultCard);\n        });\n        \n        // Update results count and header\n        this.updateResultsHeader();\n    }\n    \n    updateResultsHeader() {\n        const resultsHeader = document.querySelector('#resultsSection h2');\n        if (resultsHeader && this.results.length > 0) {\n            resultsHeader.textContent = `üìä ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿ±ÿ¨ÿ© (${this.results.length} ÿµŸÅÿ≠ÿ©)`;\n        }\n    }\n    \n    showResults() {\n        const resultsSection = document.getElementById('resultsSection');\n        resultsSection.style.display = 'block';\n    }\n    \n    hideResults() {\n        const resultsSection = document.getElementById('resultsSection');\n        resultsSection.style.display = 'none';\n    }\n    \n    async startUnlimitedStreamingScraping(url, timeout) {\n        const submitBtn = document.getElementById('submitBtn');\n        const progressSection = document.getElementById('progressSection');\n        const resultsSection = document.getElementById('resultsSection');\n        \n        try {\n            // Update UI to loading state\n            submitBtn.classList.add('loading');\n            this.showProgress();\n            this.hideResults();\n            \n            // Clear previous results\n            this.results = [];\n            \n            // Show progress section\n            progressSection.style.display = 'block';\n            progressSection.scrollIntoView({ behavior: 'smooth' });\n            \n            // Prepare unlimited streaming request\n            const streamUrl = `${this.apiBaseUrl}/scrape-stream-unlimited`;\n            const requestBody = {\n                url: url,\n                timeout: timeout\n            };\n            \n            // Start streaming with fetch and response.body.getReader()\n            const response = await fetch(streamUrl, {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                },\n                body: JSON.stringify(requestBody)\n            });\n            \n            if (!response.ok) {\n                throw new Error(`HTTP error! status: ${response.status}`);\n            }\n            \n            // Process streaming response\n            const reader = response.body.getReader();\n            const decoder = new TextDecoder();\n            let buffer = '';\n            \n            while (true) {\n                const { done, value } = await reader.read();\n                \n                if (done) break;\n                \n                // Decode the chunk and add to buffer\n                buffer += decoder.decode(value, { stream: true });\n                \n                // Process complete lines from buffer\n                const lines = buffer.split('\\n');\n                buffer = lines.pop(); // Keep incomplete line in buffer\n                \n                for (const line of lines) {\n                    if (line.trim() && line.startsWith('data: ')) {\n                        try {\n                            const jsonStr = line.substring(6); // Remove \"data: \" prefix\n                            const data = JSON.parse(jsonStr);\n                            \n                            switch (data.type) {\n                                case 'start':\n                                    this.updateProgress(0, data.message);\n                                    this.showNotification('ÿ®ÿØÿ° ÿßŸÑÿ®ÿ´ ÿßŸÑŸÖÿ®ÿßÿ¥ÿ± ÿßŸÑÿ¥ÿßŸÖŸÑ ŸÑŸÑŸÖŸàŸÇÿπ', 'info');\n                                    break;\n                                    \n                                case 'page':\n                                    // Add new page to results immediately\n                                    this.results.push({ data: data.data });\n                                    \n                                    // Update progress for unlimited mode\n                                    const current = data.progress ? data.progress.current : this.results.length;\n                                    const queueSize = data.progress ? data.progress.queue_size : 0;\n                                    \n                                    this.updateProgress(null, `ÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ${current} ÿµŸÅÿ≠ÿ©... (${queueSize} ŸÅŸä ÿßŸÑÿßŸÜÿ™ÿ∏ÿßÿ±)`);\n                                    \n                                    // Update results display in real-time\n                                    this.showResults();\n                                    this.displayStreamingResults();\n                                    \n                                    break;\n                                    \n                                case 'warning':\n                                    this.showNotification(data.message, 'warning');\n                                    break;\n                                    \n                                case 'progress':\n                                    // Update progress for milestone notifications\n                                    const currentCount = data.current || this.results.length;\n                                    this.updateProgress(null, data.message);\n                                    break;\n                                    \n                                case 'complete':\n                                    this.updateProgress(100, data.message);\n                                    this.showNotification(`ÿ™ŸÖ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ° ŸÖŸÜ ÿßŸÑÿ≥ŸÉÿ±ÿßÿ®ŸÜÿ¨ ÿßŸÑÿ¥ÿßŸÖŸÑ! ÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ${data.total_pages} ÿµŸÅÿ≠ÿ©`, 'success');\n                                    \n                                    // Final results display\n                                    this.showResults();\n                                    this.displayResults(this.results);\n                                    \n                                    return; // Exit the function\n                                    \n                                case 'error':\n                                    throw new Error(data.message);\n                            }\n                        } catch (parseError) {\n                            console.error('Error parsing unlimited stream data:', parseError);\n                        }\n                    }\n                }\n            }\n            \n        } catch (error) {\n            this.showNotification(error.message || 'ÿÆÿ∑ÿ£ ŸÅŸä ÿ®ÿØÿ° ÿßŸÑÿ®ÿ´ ÿßŸÑŸÖÿ®ÿßÿ¥ÿ± ÿßŸÑÿ¥ÿßŸÖŸÑ', 'error');\n            console.error('Unlimited streaming error:', error);\n        } finally {\n            submitBtn.classList.remove('loading');\n        }\n    }\n    \n    downloadResults() {\n        if (this.results.length === 0) {\n            this.showNotification('ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÜÿ™ÿßÿ¶ÿ¨ ŸÑŸÑÿ™ÿ≠ŸÖŸäŸÑ', 'warning');\n            return;\n        }\n        \n        const dataStr = JSON.stringify(this.results, null, 2);\n        const dataBlob = new Blob([dataStr], { type: 'application/json' });\n        \n        const link = document.createElement('a');\n        link.href = URL.createObjectURL(dataBlob);\n        link.download = `scraping-results-${new Date().toISOString().slice(0, 10)}.json`;\n        \n        document.body.appendChild(link);\n        link.click();\n        document.body.removeChild(link);\n        \n        this.showNotification('ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ŸÜÿ¨ÿßÿ≠', 'success');\n    }\n    \n    sendToDatabase() {\n        if (this.results.length === 0) {\n            this.showNotification('ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÜÿ™ÿßÿ¶ÿ¨ ŸÑÿ•ÿ±ÿ≥ÿßŸÑŸáÿß', 'warning');\n            return;\n        }\n\n        // Store results in localStorage for database page\n        localStorage.setItem('scraping_results', JSON.stringify(this.results));\n        \n        // Open database page in new tab\n        const dbWindow = window.open('/database', '_blank');\n        \n        // Show notification\n        this.showNotification(`ÿ™ŸÖ ÿ™ÿ≠ÿ∂Ÿäÿ± ${this.results.length} ŸÜÿ™Ÿäÿ¨ÿ© ŸÑŸÑÿ•ÿ±ÿ≥ÿßŸÑ ŸÑŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™`, 'success');\n    }\n\n    clearResults() {\n        if (this.results.length === 0) {\n            this.showNotification('ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÜÿ™ÿßÿ¶ÿ¨ ŸÑŸÑŸÖÿ≥ÿ≠', 'warning');\n            return;\n        }\n        \n        this.results = [];\n        this.hideResults();\n        this.hideProgress();\n        \n        const resultsGrid = document.getElementById('resultsGrid');\n        resultsGrid.innerHTML = '';\n        \n        this.showNotification('ÿ™ŸÖ ŸÖÿ≥ÿ≠ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨', 'success');\n    }\n    \n    showNotification(message, type = 'info') {\n        const container = document.getElementById('notificationContainer');\n        \n        const notification = document.createElement('div');\n        notification.className = `notification ${type}`;\n        notification.textContent = message;\n        \n        container.appendChild(notification);\n        \n        // Auto remove after 5 seconds\n        setTimeout(() => {\n            notification.style.animation = 'slideOutRight 0.3s ease forwards';\n            setTimeout(() => {\n                if (notification.parentNode) {\n                    notification.parentNode.removeChild(notification);\n                }\n            }, 300);\n        }, 5000);\n        \n        // Click to dismiss\n        notification.addEventListener('click', () => {\n            notification.remove();\n        });\n    }\n}\n\n// CSS for animations and cyber effects\nconst style = document.createElement('style');\nstyle.textContent = `\n    @keyframes fadeIn {\n        from { opacity: 0; }\n        to { opacity: 1; }\n    }\n    \n    @keyframes slideOutRight {\n        from {\n            opacity: 1;\n            transform: translateX(0) perspective(1000px) rotateY(-10deg);\n        }\n        to {\n            opacity: 0;\n            transform: translateX(100%) perspective(1000px) rotateY(-10deg);\n        }\n    }\n    \n    @keyframes rippleEffect {\n        from {\n            width: 20px;\n            height: 20px;\n            opacity: 0.8;\n        }\n        to {\n            width: 200px;\n            height: 200px;\n            opacity: 0;\n        }\n    }\n    \n    .result-card {\n        animation: slideInUp 0.5s ease both;\n    }\n    \n    .content-modal * {\n        direction: rtl;\n        text-align: right;\n    }\n    \n    .content-modal ::-webkit-scrollbar {\n        width: 6px;\n    }\n    \n    .content-modal ::-webkit-scrollbar-track {\n        background: rgba(255, 255, 255, 0.1);\n        border-radius: 3px;\n    }\n    \n    .content-modal ::-webkit-scrollbar-thumb {\n        background: linear-gradient(135deg, #667eea, #764ba2);\n        border-radius: 3px;\n    }\n`;\ndocument.head.appendChild(style);\n\n// Initialize the application\ndocument.addEventListener('DOMContentLoaded', () => {\n    window.webScrapingUI = new WebScrapingUI();\n});\n\n// Export for potential external use\nif (typeof module !== 'undefined' && module.exports) {\n    module.exports = WebScrapingUI;\n}","size_bytes":35914},"static/styles.css":{"content":"/* Modern 3D UI Design for Web Scraping API */\n\n* {\n    margin: 0;\n    padding: 0;\n    box-sizing: border-box;\n}\n\n:root {\n    /* Elegant & Calm Color Palette */\n    --primary: #d4af37;\n    --primary-dark: #b8941f;\n    --secondary: #2c3e50;\n    --accent: #ecf0f1;\n    --success: #27ae60;\n    --warning: #f39c12;\n    --error: #e74c3c;\n    --dark: #1a1a1a;\n    --dark-light: #2d2d30;\n    --dark-medium: #252526;\n    --gray: #8a8a8a;\n    --gray-light: #e8e8e8;\n    --white: #ffffff;\n    --gold: #d4af37;\n    --silver: #c0c0c0;\n    --charcoal: #36393f;\n    \n    /* Elegant Gradients */\n    --gradient-primary: linear-gradient(135deg, #d4af37 0%, #b8941f 100%);\n    --gradient-secondary: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);\n    --gradient-success: linear-gradient(135deg, #27ae60 0%, #2ecc71 100%);\n    --gradient-dark: linear-gradient(135deg, #1a1a1a 0%, #2d2d30 100%);\n    --gradient-elegant: linear-gradient(135deg, #d4af37 0%, #c0c0c0 100%);\n    --gradient-subtle: linear-gradient(135deg, #36393f 0%, #2c3e50 100%);\n    \n    /* Elegant Shadows */\n    --shadow-sm: 0 2px 8px rgba(0, 0, 0, 0.3);\n    --shadow-md: 0 4px 16px rgba(0, 0, 0, 0.4);\n    --shadow-lg: 0 8px 32px rgba(0, 0, 0, 0.5);\n    --shadow-xl: 0 16px 40px rgba(0, 0, 0, 0.6);\n    --shadow-elegant: 0 8px 32px rgba(212, 175, 55, 0.2);\n    --glow-gold: 0 0 10px rgba(212, 175, 55, 0.3), 0 0 20px rgba(212, 175, 55, 0.2);\n    --glow-subtle: 0 0 15px rgba(255, 255, 255, 0.1);\n    \n    /* Spacing */\n    --space-xs: 0.5rem;\n    --space-sm: 0.75rem;\n    --space-md: 1rem;\n    --space-lg: 1.5rem;\n    --space-xl: 2rem;\n    --space-2xl: 3rem;\n    \n    /* Border Radius */\n    --radius-sm: 8px;\n    --radius-md: 12px;\n    --radius-lg: 16px;\n    --radius-xl: 20px;\n    --radius-full: 50%;\n    \n    /* Typography */\n    --font-size-xs: 0.75rem;\n    --font-size-sm: 0.875rem;\n    --font-size-base: 1rem;\n    --font-size-lg: 1.125rem;\n    --font-size-xl: 1.25rem;\n    --font-size-2xl: 1.5rem;\n    --font-size-3xl: 1.875rem;\n    --font-size-4xl: 2.25rem;\n}\n\nbody {\n    font-family: 'Cairo', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n    background: var(--gradient-dark);\n    color: var(--white);\n    line-height: 1.6;\n    overflow-x: hidden;\n    min-height: 100vh;\n    position: relative;\n}\n\n/* Subtle Pattern Background */\nbody::before {\n    content: '';\n    position: fixed;\n    top: 0;\n    left: 0;\n    width: 100%;\n    height: 100%;\n    background: \n        radial-gradient(circle at 20% 80%, rgba(212, 175, 55, 0.05) 0%, transparent 50%),\n        radial-gradient(circle at 80% 20%, rgba(192, 192, 192, 0.03) 0%, transparent 50%);\n    z-index: -1;\n    animation: subtleShift 30s ease-in-out infinite;\n}\n\n@keyframes subtleShift {\n    0%, 100% { \n        opacity: 0.3;\n        transform: translateX(0) translateY(0);\n    }\n    50% { \n        opacity: 0.5;\n        transform: translateX(10px) translateY(-5px);\n    }\n}\n\n.container {\n    max-width: 1200px;\n    margin: 0 auto;\n    padding: var(--space-md);\n    position: relative;\n    z-index: 1;\n}\n\n/* Header */\n.header {\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    margin-bottom: var(--space-2xl);\n    padding: var(--space-lg);\n    background: var(--gradient-subtle);\n    backdrop-filter: blur(20px);\n    border-radius: var(--radius-xl);\n    border: 1px solid var(--gold);\n    box-shadow: var(--shadow-elegant);\n    transition: all 0.3s ease;\n}\n\n.header:hover {\n    transform: translateY(-2px);\n    box-shadow: var(--glow-gold), var(--shadow-xl);\n    border-color: var(--primary);\n}\n\n.logo {\n    display: flex;\n    align-items: center;\n    gap: var(--space-md);\n}\n\n.logo-icon {\n    width: 50px;\n    height: 50px;\n    background: var(--gradient-elegant);\n    border-radius: var(--radius-md);\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    font-size: var(--font-size-xl);\n    box-shadow: var(--shadow-elegant);\n    transition: all 0.3s ease;\n}\n\n.logo-icon:hover {\n    transform: scale(1.05) rotate(5deg);\n    box-shadow: var(--glow-gold);\n}\n\n@keyframes logoFloat {\n    0%, 100% { transform: translateY(0px); }\n    50% { transform: translateY(-3px); }\n}\n\n.logo h1 {\n    font-size: var(--font-size-3xl);\n    font-weight: 700;\n    background: var(--gradient-elegant);\n    -webkit-background-clip: text;\n    -webkit-text-fill-color: transparent;\n    background-clip: text;\n    transition: all 0.3s ease;\n}\n\n.logo h1:hover {\n    filter: drop-shadow(0 0 8px rgba(212, 175, 55, 0.4));\n}\n\n.status-indicator {\n    display: flex;\n    align-items: center;\n    gap: var(--space-sm);\n    padding: var(--space-sm) var(--space-md);\n    background: var(--charcoal);\n    border-radius: var(--radius-full);\n    border: 1px solid var(--silver);\n    backdrop-filter: blur(10px);\n    box-shadow: var(--shadow-sm);\n    transition: all 0.3s ease;\n}\n\n.status-indicator:hover {\n    transform: scale(1.02);\n    box-shadow: var(--glow-subtle);\n    border-color: var(--gold);\n}\n\n.status-dot {\n    width: 12px;\n    height: 12px;\n    border-radius: var(--radius-full);\n    background: var(--success);\n    box-shadow: 0 0 8px rgba(39, 174, 96, 0.4);\n    animation: pulse 2s infinite;\n}\n\n.status-dot.checking {\n    background: var(--warning);\n    box-shadow: 0 0 8px rgba(243, 156, 18, 0.4);\n}\n\n.status-dot.error {\n    background: var(--error);\n    box-shadow: 0 0 8px rgba(231, 76, 60, 0.4);\n}\n\n@keyframes pulse {\n    0% { transform: scale(1); opacity: 1; }\n    50% { transform: scale(1.1); opacity: 0.8; }\n    100% { transform: scale(1); opacity: 1; }\n}\n\n/* Main Content */\n.main-content {\n    display: flex;\n    flex-direction: column;\n    gap: var(--space-2xl);\n}\n\n/* Control Panel */\n.control-panel {\n    background: var(--gradient-subtle);\n    backdrop-filter: blur(20px);\n    border-radius: var(--radius-xl);\n    border: 1px solid var(--gold);\n    padding: var(--space-2xl);\n    box-shadow: var(--shadow-elegant);\n    transition: all 0.3s ease;\n}\n\n.control-panel:hover {\n    transform: translateY(-3px);\n    box-shadow: var(--glow-gold), var(--shadow-xl);\n    border-color: var(--primary);\n}\n\n.panel-header {\n    text-align: center;\n    margin-bottom: var(--space-2xl);\n}\n\n.panel-header h2 {\n    font-size: var(--font-size-3xl);\n    font-weight: 700;\n    margin-bottom: var(--space-sm);\n    color: var(--gold);\n    text-shadow: 0 0 10px rgba(212, 175, 55, 0.3);\n    transition: all 0.3s ease;\n}\n\n.panel-header h2:hover {\n    text-shadow: 0 0 15px rgba(212, 175, 55, 0.5);\n    transform: scale(1.02);\n}\n\n.panel-header p {\n    color: var(--gray-light);\n    font-size: var(--font-size-lg);\n    font-style: italic;\n    opacity: 0.9;\n}\n\n/* Form */\n.form-container {\n    max-width: 600px;\n    margin: 0 auto;\n}\n\n.scrape-form {\n    display: flex;\n    flex-direction: column;\n    gap: var(--space-xl);\n}\n\n.input-group {\n    display: flex;\n    flex-direction: column;\n    gap: var(--space-sm);\n}\n\n.input-group label {\n    font-weight: 600;\n    color: var(--white);\n    font-size: var(--font-size-lg);\n    margin-bottom: var(--space-sm);\n    display: block;\n    transition: all 0.3s ease;\n}\n\n.input-group:focus-within label {\n    color: var(--gold);\n    text-shadow: 0 0 5px rgba(212, 175, 55, 0.3);\n}\n\n.input-group input {\n    padding: var(--space-md) var(--space-lg);\n    background: var(--charcoal);\n    border: 2px solid var(--silver);\n    border-radius: var(--radius-md);\n    color: var(--white);\n    font-size: var(--font-size-base);\n    transition: all 0.3s ease;\n    backdrop-filter: blur(10px);\n}\n\n.input-group input:focus {\n    outline: none;\n    border-color: var(--gold);\n    box-shadow: var(--glow-gold);\n    transform: translateY(-1px);\n    background: var(--dark-medium);\n}\n\n.input-group input:hover {\n    border-color: var(--primary);\n    box-shadow: var(--glow-subtle);\n}\n\n.input-group input::placeholder {\n    color: rgba(255, 255, 255, 0.5);\n}\n\n.input-hint {\n    font-size: var(--font-size-sm);\n    color: var(--silver);\n    font-style: italic;\n    margin-top: var(--space-xs);\n    transition: all 0.3s ease;\n    opacity: 0.8;\n}\n\n.input-group:focus-within .input-hint {\n    color: var(--gold);\n    opacity: 1;\n}\n\n.settings-grid {\n    display: grid;\n    grid-template-columns: 1fr 1fr;\n    gap: var(--space-lg);\n}\n\n/* Submit Button */\n.submit-btn {\n    position: relative;\n    padding: var(--space-lg) var(--space-2xl);\n    background: var(--gradient-elegant);\n    border: none;\n    border-radius: var(--radius-md);\n    color: var(--dark);\n    font-size: var(--font-size-lg);\n    font-weight: 700;\n    cursor: pointer;\n    transition: all 0.3s ease;\n    box-shadow: var(--shadow-elegant);\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    gap: var(--space-sm);\n    text-transform: uppercase;\n    letter-spacing: 1px;\n}\n\n.submit-btn:hover {\n    transform: translateY(-3px) scale(1.02);\n    box-shadow: var(--glow-gold), var(--shadow-xl);\n}\n\n.submit-btn:active {\n    transform: translateY(-1px) scale(1.01);\n}\n\n.submit-btn.loading {\n    pointer-events: none;\n}\n\n.btn-loader {\n    width: 20px;\n    height: 20px;\n    border: 2px solid rgba(255, 255, 255, 0.3);\n    border-top: 2px solid var(--white);\n    border-radius: var(--radius-full);\n    animation: spin 1s linear infinite;\n    display: none;\n}\n\n.submit-btn.loading .btn-loader {\n    display: block;\n}\n\n.submit-btn.loading .btn-text,\n.submit-btn.loading .btn-icon {\n    display: none;\n}\n\n@keyframes spin {\n    0% { transform: rotate(0deg); }\n    100% { transform: rotate(360deg); }\n}\n\n/* Progress Section */\n.progress-section {\n    background: var(--gradient-subtle);\n    backdrop-filter: blur(20px);\n    border-radius: var(--radius-xl);\n    border: 1px solid var(--gold);\n    padding: var(--space-xl);\n    box-shadow: var(--shadow-elegant);\n    animation: slideInUp 0.5s ease;\n    transition: all 0.3s ease;\n}\n\n.progress-section:hover {\n    box-shadow: var(--glow-gold), var(--shadow-xl);\n    border-color: var(--primary);\n}\n\n@keyframes slideInUp {\n    from {\n        opacity: 0;\n        transform: translateY(30px);\n    }\n    to {\n        opacity: 1;\n        transform: translateY(0);\n    }\n}\n\n.progress-header {\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    margin-bottom: var(--space-lg);\n}\n\n.progress-header h3 {\n    font-size: var(--font-size-2xl);\n    font-weight: 600;\n}\n\n.progress-bar-container {\n    background: rgba(255, 255, 255, 0.1);\n    border-radius: var(--radius-full);\n    height: 12px;\n    overflow: hidden;\n    position: relative;\n}\n\n.progress-bar {\n    width: 100%;\n    height: 100%;\n    position: relative;\n}\n\n.progress-fill {\n    height: 100%;\n    background: var(--gradient-elegant);\n    border-radius: var(--radius-full);\n    width: 0%;\n    transition: width 0.3s ease;\n    position: relative;\n    overflow: hidden;\n    box-shadow: 0 0 10px rgba(212, 175, 55, 0.3);\n}\n\n.progress-fill::after {\n    content: '';\n    position: absolute;\n    top: 0;\n    left: 0;\n    bottom: 0;\n    right: 0;\n    background: linear-gradient(\n        90deg,\n        transparent,\n        rgba(255, 255, 255, 0.4),\n        transparent\n    );\n    animation: shimmer 2s infinite;\n}\n\n@keyframes shimmer {\n    0% { transform: translateX(-100%); }\n    100% { transform: translateX(100%); }\n}\n\n/* Results Section */\n.results-section {\n    background: var(--gradient-subtle);\n    backdrop-filter: blur(20px);\n    border-radius: var(--radius-xl);\n    border: 1px solid var(--silver);\n    padding: var(--space-xl);\n    box-shadow: var(--shadow-lg);\n    animation: slideInUp 0.5s ease;\n    transition: all 0.3s ease;\n}\n\n.results-section:hover {\n    border-color: var(--gold);\n    box-shadow: var(--glow-subtle), var(--shadow-xl);\n}\n\n.results-header {\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    margin-bottom: var(--space-xl);\n}\n\n.results-header h3 {\n    font-size: var(--font-size-2xl);\n    font-weight: 600;\n}\n\n.results-actions {\n    display: flex;\n    gap: var(--space-sm);\n}\n\n.action-btn {\n    padding: var(--space-sm) var(--space-md);\n    background: var(--charcoal);\n    border: 1px solid var(--silver);\n    border-radius: var(--radius-sm);\n    color: var(--white);\n    cursor: pointer;\n    transition: all 0.3s ease;\n    font-size: var(--font-size-sm);\n    backdrop-filter: blur(10px);\n    font-weight: 600;\n}\n\n.action-btn:hover {\n    background: var(--dark-medium);\n    border-color: var(--gold);\n    color: var(--gold);\n    transform: translateY(-2px);\n    box-shadow: var(--glow-gold);\n    text-shadow: 0 0 8px rgba(212, 175, 55, 0.3);\n}\n\n.results-grid {\n    display: grid;\n    grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));\n    gap: var(--space-lg);\n}\n\n.result-card {\n    background: var(--charcoal);\n    border: 1px solid var(--silver);\n    border-radius: var(--radius-md);\n    padding: var(--space-lg);\n    transition: all 0.3s ease;\n    backdrop-filter: blur(10px);\n    cursor: pointer;\n}\n\n.result-card:hover {\n    transform: translateY(-4px);\n    box-shadow: var(--glow-gold), var(--shadow-xl);\n    border-color: var(--gold);\n    background: var(--dark-medium);\n}\n\n.result-card h4 {\n    color: var(--accent);\n    font-size: var(--font-size-lg);\n    font-weight: 600;\n    margin-bottom: var(--space-sm);\n    display: -webkit-box;\n    -webkit-line-clamp: 2;\n    -webkit-box-orient: vertical;\n    overflow: hidden;\n}\n\n.result-card .url {\n    color: var(--primary);\n    font-size: var(--font-size-sm);\n    margin-bottom: var(--space-md);\n    word-break: break-all;\n}\n\n.result-card .content {\n    color: var(--gray-light);\n    font-size: var(--font-size-sm);\n    line-height: 1.5;\n    display: -webkit-box;\n    -webkit-line-clamp: 4;\n    -webkit-box-orient: vertical;\n    overflow: hidden;\n}\n\n.result-card .meta {\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    margin-top: var(--space-md);\n    padding-top: var(--space-md);\n    border-top: 1px solid rgba(255, 255, 255, 0.1);\n    font-size: var(--font-size-xs);\n    color: var(--gray);\n}\n\n/* API Info */\n.api-info {\n    background: var(--gradient-subtle);\n    backdrop-filter: blur(20px);\n    border-radius: var(--radius-xl);\n    border: 1px solid var(--silver);\n    padding: var(--space-xl);\n    box-shadow: var(--shadow-lg);\n    transition: all 0.3s ease;\n}\n\n.api-info:hover {\n    border-color: var(--gold);\n    box-shadow: var(--glow-subtle), var(--shadow-xl);\n}\n\n.info-header h3 {\n    font-size: var(--font-size-2xl);\n    font-weight: 600;\n    margin-bottom: var(--space-xl);\n    text-align: center;\n}\n\n.info-grid {\n    display: grid;\n    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n    gap: var(--space-lg);\n}\n\n.info-card {\n    background: var(--charcoal);\n    border: 1px solid var(--silver);\n    border-radius: var(--radius-md);\n    padding: var(--space-lg);\n    text-align: center;\n    transition: all 0.3s ease;\n    backdrop-filter: blur(10px);\n    cursor: pointer;\n}\n\n.info-card:hover {\n    transform: translateY(-4px) scale(1.02);\n    box-shadow: var(--glow-gold), var(--shadow-xl);\n    border-color: var(--gold);\n    background: var(--dark-medium);\n}\n\n.info-icon {\n    font-size: var(--font-size-4xl);\n    margin-bottom: var(--space-md);\n    display: block;\n}\n\n.info-card h4 {\n    font-size: var(--font-size-lg);\n    font-weight: 600;\n    margin-bottom: var(--space-sm);\n    color: var(--gold);\n    text-shadow: 0 0 5px rgba(212, 175, 55, 0.3);\n    transition: all 0.3s ease;\n}\n\n.info-card:hover h4 {\n    text-shadow: 0 0 10px rgba(212, 175, 55, 0.5);\n}\n\n.info-card code {\n    background: var(--dark-medium);\n    padding: var(--space-xs) var(--space-sm);\n    border-radius: var(--radius-sm);\n    font-family: 'Courier New', monospace;\n    font-size: var(--font-size-sm);\n    color: var(--gold);\n    text-shadow: 0 0 5px rgba(212, 175, 55, 0.3);\n    border: 1px solid var(--silver);\n}\n\n.info-card a {\n    color: var(--gold);\n    text-decoration: none;\n    font-weight: 500;\n    transition: all 0.3s ease;\n    text-shadow: 0 0 5px rgba(212, 175, 55, 0.2);\n}\n\n.info-card a:hover {\n    color: var(--primary);\n    text-shadow: 0 0 10px rgba(212, 175, 55, 0.5);\n}\n\n/* Footer */\n.footer {\n    text-align: center;\n    margin-top: var(--space-2xl);\n    padding: var(--space-xl);\n    color: var(--gray);\n    font-size: var(--font-size-sm);\n    position: relative;\n    opacity: 0.8;\n}\n\n.footer::before {\n    content: '';\n    position: absolute;\n    top: 0;\n    left: 50%;\n    transform: translateX(-50%);\n    width: 100px;\n    height: 1px;\n    background: var(--gradient-elegant);\n    opacity: 0.5;\n}\n\n/* Notifications */\n.notification-container {\n    position: fixed;\n    top: 20px;\n    right: 20px;\n    z-index: 1000;\n    display: flex;\n    flex-direction: column;\n    gap: var(--space-sm);\n}\n\n.notification {\n    background: var(--charcoal);\n    backdrop-filter: blur(20px);\n    color: var(--white);\n    padding: var(--space-md) var(--space-lg);\n    border-radius: var(--radius-md);\n    border: 1px solid var(--gold);\n    box-shadow: var(--shadow-elegant);\n    max-width: 350px;\n    animation: slideInRight 0.3s ease;\n    transition: all 0.3s ease;\n    position: relative;\n    overflow: hidden;\n}\n\n.notification::before {\n    content: '';\n    position: absolute;\n    top: 0;\n    left: 0;\n    width: 100%;\n    height: 3px;\n    background: var(--gradient-elegant);\n    animation: notificationProgress 5s linear;\n}\n\n@keyframes notificationProgress {\n    0% { width: 100%; }\n    100% { width: 0%; }\n}\n\n.notification:hover {\n    transform: scale(1.02);\n    box-shadow: var(--glow-gold), var(--shadow-xl);\n}\n\n.notification.success {\n    border-color: var(--success);\n}\n\n.notification.error {\n    border-color: var(--error);\n}\n\n.notification.warning {\n    border-color: var(--warning);\n}\n\n@keyframes slideInRight {\n    from {\n        opacity: 0;\n        transform: translateX(100%) perspective(1000px) rotateY(-10deg);\n    }\n    to {\n        opacity: 1;\n        transform: translateX(0) perspective(1000px) rotateY(-10deg);\n    }\n}\n\n/* Responsive Design */\n@media (max-width: 768px) {\n    .container {\n        padding: var(--space-sm);\n    }\n    \n    .header {\n        flex-direction: column;\n        gap: var(--space-md);\n        text-align: center;\n    }\n    \n    .logo {\n        justify-content: center;\n    }\n    \n    .settings-grid {\n        grid-template-columns: 1fr;\n    }\n    \n    .results-actions {\n        flex-direction: column;\n    }\n    \n    .info-grid {\n        grid-template-columns: 1fr;\n    }\n    \n    .notification-container {\n        left: 10px;\n        right: 10px;\n    }\n    \n    .notification {\n        max-width: none;\n    }\n}\n\n@media (max-width: 480px) {\n    .logo h1 {\n        font-size: var(--font-size-2xl);\n    }\n    \n    .panel-header h2 {\n        font-size: var(--font-size-2xl);\n    }\n    \n    .results-grid {\n        grid-template-columns: 1fr;\n    }\n}\n\n/* Custom Scrollbar */\n::-webkit-scrollbar {\n    width: 8px;\n}\n\n::-webkit-scrollbar-track {\n    background: var(--charcoal);\n    border-radius: var(--radius-sm);\n}\n\n::-webkit-scrollbar-thumb {\n    background: var(--gradient-elegant);\n    border-radius: var(--radius-sm);\n    box-shadow: 0 0 5px rgba(212, 175, 55, 0.3);\n}\n\n::-webkit-scrollbar-thumb:hover {\n    background: var(--gold);\n    box-shadow: var(--glow-gold);\n}\n\n/* Selection */\n::selection {\n    background: rgba(212, 175, 55, 0.3);\n    color: var(--white);\n    text-shadow: 0 0 5px rgba(212, 175, 55, 0.5);\n}\n\n::-moz-selection {\n    background: rgba(212, 175, 55, 0.3);\n    color: var(--white);\n    text-shadow: 0 0 5px rgba(212, 175, 55, 0.5);\n}\n\n/* Mode Selection Styles */\n.mode-selection {\n    margin-bottom: 2rem;\n    padding: 1.5rem;\n    background: var(--gradient-subtle);\n    border: 1px solid var(--silver);\n    border-radius: 16px;\n    backdrop-filter: blur(15px);\n    transition: all 0.3s ease;\n}\n\n.mode-selection:hover {\n    border-color: var(--gold);\n    box-shadow: var(--glow-subtle);\n}\n\n.mode-label {\n    display: block;\n    font-size: 1.1rem;\n    font-weight: 700;\n    color: var(--gold);\n    margin-bottom: 1rem;\n    text-align: center;\n    text-shadow: 0 0 8px rgba(212, 175, 55, 0.3);\n}\n\n.radio-group {\n    display: flex;\n    flex-direction: column;\n    gap: 1rem;\n}\n\n.radio-option {\n    display: flex;\n    align-items: center;\n    gap: 0.75rem;\n    padding: 1rem;\n    background: var(--charcoal);\n    border: 2px solid var(--silver);\n    border-radius: 12px;\n    cursor: pointer;\n    transition: all 0.3s ease;\n    backdrop-filter: blur(10px);\n}\n\n.radio-option:hover {\n    border-color: var(--gold);\n    transform: translateY(-2px);\n    box-shadow: var(--glow-gold);\n    background: var(--dark-medium);\n}\n\n.radio-option input[type=\"radio\"] {\n    display: none;\n}\n\n.radio-custom {\n    width: 20px;\n    height: 20px;\n    border: 2px solid rgba(255, 255, 255, 0.3);\n    border-radius: 50%;\n    position: relative;\n    transition: all 0.3s ease;\n    z-index: 1;\n}\n\n.radio-custom::after {\n    content: '';\n    width: 10px;\n    height: 10px;\n    background: var(--gradient-elegant);\n    border-radius: 50%;\n    position: absolute;\n    top: 50%;\n    left: 50%;\n    transform: translate(-50%, -50%) scale(0);\n    transition: all 0.3s ease;\n    box-shadow: 0 0 8px rgba(212, 175, 55, 0.5);\n}\n\n.radio-option input[type=\"radio\"]:checked + .radio-custom {\n    border-color: var(--gold);\n    box-shadow: var(--glow-gold);\n}\n\n.radio-option input[type=\"radio\"]:checked + .radio-custom::after {\n    transform: translate(-50%, -50%) scale(1);\n}\n\n.radio-option input[type=\"radio\"]:checked ~ .radio-text {\n    color: var(--gold);\n    font-weight: 700;\n    text-shadow: 0 0 8px rgba(212, 175, 55, 0.3);\n}\n\n.radio-text {\n    font-size: 1rem;\n    color: rgba(255, 255, 255, 0.9);\n    transition: all 0.3s ease;\n    z-index: 1;\n    flex: 1;\n}\n\n/* Responsive Mode Selection */\n@media (min-width: 768px) {\n    .radio-group {\n        flex-direction: row;\n        gap: 1rem;\n    }\n    \n    .radio-option {\n        flex: 1;\n        justify-content: center;\n        text-align: center;\n    }\n}\n\n@media (max-width: 768px) {\n    .radio-group {\n        gap: 0.75rem;\n    }\n    \n    .radio-option {\n        padding: 0.75rem;\n    }\n    \n    .radio-text {\n        font-size: 0.9rem;\n    }\n}","size_bytes":22090}}}